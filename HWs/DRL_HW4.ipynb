{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88af3281",
   "metadata": {},
   "source": [
    "# HW 4 - Inverse RL & GRPO Fine-Tuning\n",
    "\n",
    "Reward modeling is the glue between human supervision and scalable RL systems. Classical inverse reinforcement learning (IRL) lets us infer reward functions directly from expert demonstrations, and that same idea underpins modern training pipelines (r.g. RLHF, RLAIF, Constitutional AI) where we fit reward models or preference models for large language models. Even though today's top AI labs rely on pairwise/ranked preference data or verifiable rewards rather than tabular MaxEnt IRL, the maximum entropy view is a foundational recipe: it teaches us to match feature expectations while keeping the trajectory distribution as high-entropy (i.e., as uncertain) as possible away from labeled data, which is a philosophy we still enforce when training modern reward models.\n",
    "\n",
    "In **Section 1**, we will apply Maximum Entropy Inverse Reinforcement Learning (MaxEnt IRL) algorithm from [Ziebart et al., 2008](https://cdn.aaai.org/AAAI/2008/AAAI08-227.pdf) to recover rewards from demonstrations.\n",
    "\n",
    "In **Section 2**, we will fine-tune a QLoRA-adapted large language model with the Group Relative Policy Optimization (GRPO) algorithm from [Shao et al., 2024](https://arxiv.org/abs/2402.03300) so the LLM outputs follow structured reasoning formats.\n",
    "\n",
    "\n",
    "**Runtime requirement.** This notebook assumes access to a GPU (MaxEnt IRL visualization is light, but GRPO + QLoRA fine-tuning will not run on CPU). If you are using Google Colab, open `Runtime → Change runtime type` and set the hardware accelerator to `GPU`, then restart the runtime before continuing.\n",
    "\n",
    "To conserve GPU hours, we recommend leaving the notebook on a CPU runtime while you work through Section 1 and the early GRPO setup cells. Switch to a GPU only once you reach the GRPO training section where accelerated generation becomes necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660fc4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running on Colab (recommended)\n",
    "!pip install -U transformers datasets accelerate bitsandbytes peft math-verify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b937b9",
   "metadata": {},
   "source": [
    "## 1. Maximum Entropy IRL [40 pts]\n",
    "\n",
    "1. Conceptual questions [12 pts]\n",
    "2. MaxEnt IRL implementation [28 pts]\n",
    "\n",
    "Maximum Entropy IRL (Ziebart et al., 2008) infers a reward function that explains expert demonstrations while remaining as uncertain as possible beyond the observed behavior. By matching **feature** expectations under a stochastic policy, we recover rewards that generalize better than directly cloning actions.\n",
    "\n",
    "**Suggested resources.**\n",
    "- B. D. Ziebart et al., *Maximum Entropy Inverse Reinforcement Learning*, AAAI 2008\n",
    "- B. D. Ziebart, *Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy*, PhD thesis, 2010\n",
    "- S. Levine, *Reinforcement Learning and Control as Probabilistic Inference*, NeurIPS 2018 tutorial\n",
    "\n",
    "The exercises that follow rebuild the MaxEnt IRL pipeline from demonstrations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2670749",
   "metadata": {},
   "source": [
    "### The IRL objective\n",
    "\n",
    "We work with a Markov decision process $(S, A, T, R, \\gamma)$ where transitions $T(s_t, a_t, s_{t+1})$ and the discount $\\gamma$ are known while the per-state reward $R(s)$ is not. Reinforcement learning searches for a policy $\\pi$ that maximizes expected discounted return, but in inverse RL we only watch demonstrations $\\mathcal{D} = \\{\\tau_i\\}$ produced by an expert policy $\\pi^E$. Each trajectory $\\tau = ((s_0, a_0), \\ldots, s_T)$ is summarized through hand-crafted features $\\phi: S \\to \\mathbb{R}^d$, and we assume a linear reward $R_\\omega(s) = \\omega^\\top \\phi(s)$. The goal is to recover parameters $\\omega$ whose induced behavior explains the demonstrations as well as the original expert policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60f195a",
   "metadata": {},
   "source": [
    "### Principle of Maximum Entropy\n",
    "\n",
    "Feature-expectation matching requires the learner to visit features just as often as the expert does,\n",
    "$$\n",
    "    \\mathbb{E}_{\\pi^L}[\\phi(\\tau)] = \\mathbb{E}_{\\pi^E}[\\phi(\\tau)],\n",
    "$$\n",
    "where the learner's policy $\\pi^L$ emerges from the trajectory distribution $p(\\tau)$. This constraint alone leaves infinitely many rewards that can explain $\\mathcal{D}$. Following Jaynes (1957) and Ziebart et al. (2008), we pick the solution with maximum entropy so that we add no extra bias beyond the data we observed. Intuitively, higher-entropy distributions encode fewer additional assumptions—**if two policies both satisfy the feature constraints, we prefer the one whose trajectories remain as uncertain as possible**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e6fd9a",
   "metadata": {},
   "source": [
    "### Trajectory distributions, gradients, and state visitation frequencies\n",
    "\n",
    "Applying Lagrange multipliers yields a trajectory distribution of the form\n",
    "$$\n",
    "    p(\\tau \\mid \\omega) = \\frac{1}{Z(\\omega)} \\exp(\\omega^\\top \\phi(\\tau)) \\prod_{t} p(s_{t+1} \\mid s_t, a_t),\n",
    "$$\n",
    "with partition function $Z(\\omega)$ ensuring normalization. Maximizing the log-likelihood of the demonstrations leads to the gradient\n",
    "$$\n",
    "    \\nabla_\\omega \\mathcal{L}(\\omega) = \\mu_E - \\sum_s D_s\\, \\phi(s),\n",
    "$$\n",
    "where $\\mu_E$ is the empirical feature expectation and $D_s$ is the time-aggregated expected state-visitation frequency (sum over all time steps) under the current reward. In the forward recursion below we will write $d_t(s)$ for the per-time-step visitation frequency at horizon index $t$, so that $D_s = \\sum_t d_t(s)$. Computing $D_s$ requires a backward pass for the partition functions ($Z_{s,a}$ and $Z_s$ provide the local action probabilities $\\pi(a \\mid s) = Z_{s,a} / Z_s$) followed by a forward rollout that propagates visitation counts from the initial-state distribution. These are the exact steps implemented below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50651d83",
   "metadata": {},
   "source": [
    "### 1.a Conceptual Questions [12 pts total; 3 pts each]\n",
    "\n",
    "1. Two demonstrations $\\tau_1$ and $\\tau_2$ accumulate feature sums $F_1$ and $F_2$. Under the MaxEnt IRL distribution, write the log-odds $\\log p(\\tau_1 \\mid \\omega) - \\log p(\\tau_2 \\mid \\omega)$ and state when the odds become zero.\n",
    "\n",
    "<font color=\"blue\">\n",
    "Given\n",
    "\n",
    "$$p(\\tau \\mid \\omega) = \\frac{1}{Z(\\omega)} \\exp(\\omega^\\intercal \\phi(\\tau)) \\prod_t p(s_{t+1} \\mid s_t, a_t)$$\n",
    "\n",
    "and thus\n",
    "\n",
    "$$p(\\tau \\mid \\omega) \\propto \\exp(\\omega^\\intercal \\phi(\\tau))$$\n",
    "\n",
    "We seek to find\n",
    "\n",
    "$$\\log p(\\tau_1 \\mid \\omega) - \\log p(\\tau_2 \\mid \\omega)$$\n",
    "\n",
    "$$= \\log(\\frac{\\frac{1}{Z(\\omega)} \\exp(\\omega^\\intercal \\phi(\\tau_1)) \\prod_t p(s_{t+1} \\mid s_t, a_t)}{\\frac{1}{Z(\\omega)} \\exp(\\omega^\\intercal \\phi(\\tau_2)) \\prod_t p(s_{t+1} \\mid s_t, a_t)})$$\n",
    "\n",
    "All factors, except $\\exp(\\omega^\\intercal \\phi(\\tau))$, cancel out—leaving\n",
    "\n",
    "$$= \\log(\\frac{\\exp(\\omega^\\intercal \\phi(\\tau_1))}{\\exp(\\omega^\\intercal \\phi(\\tau_2))})$$\n",
    "\n",
    "or\n",
    "\n",
    "$$= \\log(\\exp(\\omega^\\intercal \\phi(\\tau_1))) - \\log(\\exp(\\omega^\\intercal \\phi(\\tau_2)))$$\n",
    "\n",
    "$$= \\omega^\\intercal \\phi(\\tau_1) - \\omega^\\intercal \\phi(\\tau_2)$$\n",
    "\n",
    "$$= \\omega^\\intercal (\\phi(\\tau_1) - \\phi(\\tau_2))$$\n",
    "\n",
    "$$= \\omega^\\intercal (F_1 - F_2)$$\n",
    "\n",
    "The odd becomes zero when $F_1 = F_2$.\n",
    "</font>\n",
    "\n",
    "2. Explain what goes wrong if you try to roll out expected visitation frequencies before finishing the backward partition pass.\n",
    "\n",
    "<font color=\"blue\">\n",
    "The backward partition pass serves the role of computing the value and Q-value functions (via dynamic programming), so that a downstream policy can be constructed from them. \n",
    "\n",
    "If the backward partition pass is not finished before the forward pass, the value and Q-value functions will be incorrect, and thus the forward pass (policy inference) will not correctly represent the current reward function (that is being learned). Thus, the gradients, loss, and optimization will update (feature importance) weights $\\omega$ in a way that probably does not help obtain a better reward function. \n",
    "</font>\n",
    "\n",
    "3. Provide the forward recursion for the expected visitation frequencies $d_{t+1}(s')$ **and** state how you prevent terminal states from re-emitting probability mass.\n",
    "\n",
    "<font color=\"blue\">Your answer here.</font>\n",
    "\n",
    "4. Name one numerical-stability technique for the partition or visitation computations and briefly describe how it mitigates underflow/overflow.\n",
    "\n",
    "<font color=\"blue\">\n",
    "The `log-sum-exp` trick converts floating point values into logarithmic values before multiplying them together (or adding them together, in logarithmic representation). This is because multiplying lots of small (<1.0) floating point values together can lead to super tiny numbers that may be truncated into zero. However, when turned into logarithms, the operations now become additions, so their value can be retained without worrying about truncation.\n",
    "\n",
    "This is used when computing partition functions (or soft value iteration). Since there can be lots of possible states and lots of possible actions (and even more state-action pairs), and calculating the value function requires calculating rewards across action possibilities, this becomes important.\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68269765",
   "metadata": {},
   "source": [
    "### 1.b MaxEnt IRL Implementation [28 pts]\n",
    "\n",
    "Implement the four helpers below (feature expectations, initial-state distribution, expected state visitation frequencies, and the exponentiated-gradient solver). Section 1 unit tests check them in the same order.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb1a9d4",
   "metadata": {},
   "source": [
    "### Section 1 implementation roadmap\n",
    "\n",
    "The next three code cells are boilerplate: they define the GridWorld dynamics, trajectory containers, and data-generation utilities shared by this notebook. Skim them for context, but there is no TODO or grading logic inside.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73ffbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from itertools import product\n",
    "from typing import Callable, Iterable, List\n",
    "\n",
    "# -------------------------------\n",
    "# Grid world dynamics\n",
    "# -------------------------------\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self, size: int):\n",
    "        self.size = size\n",
    "        self.actions = [(1, 0), (-1, 0), (0, 1), (0, -1)]\n",
    "        self.n_actions = len(self.actions)\n",
    "        self.n_states = size * size\n",
    "        self.p_transition = self._build_transition_table()\n",
    "\n",
    "    def state_index_to_point(self, state: int):\n",
    "        return state % self.size, state // self.size\n",
    "\n",
    "    def state_point_to_index(self, point):\n",
    "        x, y = point\n",
    "        return y * self.size + x\n",
    "\n",
    "    def _clip_point(self, point):\n",
    "        x, y = point\n",
    "        x = min(max(x, 0), self.size - 1)\n",
    "        y = min(max(y, 0), self.size - 1)\n",
    "        return x, y\n",
    "\n",
    "    def _step_intended(self, state: int, action: int):\n",
    "        x, y = self.state_index_to_point(state)\n",
    "        dx, dy = self.actions[action]\n",
    "        return self.state_point_to_index(self._clip_point((x + dx, y + dy)))\n",
    "\n",
    "    def _build_transition_table(self):\n",
    "        table = np.zeros((self.n_states, self.n_states, self.n_actions), dtype=np.float32)\n",
    "        for s_from, s_to, a in product(range(self.n_states), range(self.n_states), range(self.n_actions)):\n",
    "            table[s_from, s_to, a] = self._transition_prob(s_from, s_to, a)\n",
    "        return table\n",
    "\n",
    "    def _transition_prob(self, s_from, s_to, a):\n",
    "        fx, fy = self.state_index_to_point(s_from)\n",
    "        tx, ty = self.state_index_to_point(s_to)\n",
    "        ax, ay = self.actions[a]\n",
    "\n",
    "        # deterministic transition defined by action\n",
    "        if fx + ax == tx and fy + ay == ty:\n",
    "            return 1.0\n",
    "\n",
    "        # we can stay at the same state if we would move over an edge\n",
    "        if fx == tx and fy == ty:\n",
    "            if not 0 <= fx + ax < self.size or not 0 <= fy + ay < self.size:\n",
    "                return 1.0\n",
    "\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "class IcyGridWorld(GridWorld):\n",
    "    def __init__(self, size: int, p_slip: float):\n",
    "        self.p_slip = p_slip\n",
    "        super().__init__(size)\n",
    "\n",
    "    def _transition_prob(self, s_from, s_to, a):\n",
    "        fx, fy = self.state_index_to_point(s_from)\n",
    "        tx, ty = self.state_index_to_point(s_to)\n",
    "        ax, ay = self.actions[a]\n",
    "\n",
    "        # intended transition defined by action\n",
    "        if fx + ax == tx and fy + ay == ty:\n",
    "            return 1.0 - self.p_slip + self.p_slip / self.n_actions\n",
    "\n",
    "        # we can slip to all neighboring states\n",
    "        if abs(fx - tx) + abs(fy - ty) == 1:\n",
    "            return self.p_slip / self.n_actions\n",
    "\n",
    "        # we can stay at the same state if we would move over an edge\n",
    "        if fx == tx and fy == ty:\n",
    "            if not 0 <= fx + ax < self.size or not 0 <= fy + ay < self.size:\n",
    "                if not 0 < fx < self.size - 1 and not 0 < fy < self.size - 1:\n",
    "                    return 1.0 - self.p_slip + 2.0 * self.p_slip / self.n_actions\n",
    "                return 1.0 - self.p_slip + self.p_slip / self.n_actions\n",
    "\n",
    "            if not 0 < fx < self.size - 1 and not 0 < fy < self.size - 1:\n",
    "                return 2.0 * self.p_slip / self.n_actions\n",
    "            if not 0 < fx < self.size - 1 or not 0 < fy < self.size - 1:\n",
    "                return self.p_slip / self.n_actions\n",
    "            return 0.0\n",
    "\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def state_features(world: GridWorld) -> np.ndarray:\n",
    "    return np.eye(world.n_states, dtype=np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4e514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Trajectories and solvers\n",
    "# -------------------------------\n",
    "\n",
    "@dataclass\n",
    "class Transition:\n",
    "    s_from: int\n",
    "    action: int\n",
    "    s_to: int\n",
    "\n",
    "\n",
    "class Trajectory:\n",
    "    def __init__(self, transitions: List[Transition]):\n",
    "        self._transitions = transitions\n",
    "\n",
    "    def transitions(self) -> List[Transition]:\n",
    "        return self._transitions\n",
    "\n",
    "    def state_sequence(self) -> np.ndarray:\n",
    "        states = [tr.s_from for tr in self._transitions]\n",
    "        if self._transitions:\n",
    "            states.append(self._transitions[-1].s_to)\n",
    "        return np.asarray(states, dtype=np.int64)\n",
    "\n",
    "\n",
    "def generate_trajectory(world: GridWorld, policy_logits: np.ndarray, start: int, terminal: Iterable[int], rng: np.random.Generator) -> Trajectory:\n",
    "    state = start\n",
    "    transitions = []\n",
    "    while state not in terminal:\n",
    "        probs = policy_logits[state]\n",
    "        action = rng.choice(world.n_actions, p=probs)\n",
    "        next_state = rng.choice(world.n_states, p=world.p_transition[state, :, action])\n",
    "        transitions.append(Transition(state, action, next_state))\n",
    "        state = next_state\n",
    "    return Trajectory(transitions)\n",
    "\n",
    "\n",
    "def generate_trajectories(world: GridWorld, policy_logits: np.ndarray, start_states: np.ndarray, terminal: Iterable[int], n_trajectories: int, seed: int = 0) -> List[Trajectory]:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    demos = []\n",
    "    for _ in range(n_trajectories):\n",
    "        s0 = rng.choice(start_states)\n",
    "        demos.append(generate_trajectory(world, policy_logits, s0, terminal, rng))\n",
    "    return demos\n",
    "\n",
    "\n",
    "def value_iteration(p_transition: np.ndarray, reward: np.ndarray, discount: float, eps: float = 1e-4) -> np.ndarray:\n",
    "    n_states, _, n_actions = p_transition.shape\n",
    "    v = np.zeros(n_states, dtype=np.float32)\n",
    "    delta = np.inf\n",
    "    while delta > eps:\n",
    "        v_old = v.copy()\n",
    "        q = np.zeros((n_states, n_actions), dtype=np.float32)\n",
    "        for a in range(n_actions):\n",
    "            q[:, a] = reward + discount * p_transition[:, :, a] @ v\n",
    "        v = q.max(axis=1)\n",
    "        delta = np.max(np.abs(v - v_old))\n",
    "    return v\n",
    "\n",
    "\n",
    "def stochastic_policy_from_value(world: GridWorld, value: np.ndarray, weighting: Callable[[float], float] = lambda x: x) -> np.ndarray:\n",
    "    softened = np.exp(value.astype(np.float64))\n",
    "    prefs = np.zeros((world.n_states, world.n_actions), dtype=np.float64)\n",
    "    for s in range(world.n_states):\n",
    "        for a in range(world.n_actions):\n",
    "            intended = world._step_intended(s, a)\n",
    "            prefs[s, a] = weighting(softened[intended])\n",
    "    row_sums = prefs.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0.0] = 1.0\n",
    "    return (prefs / row_sums).astype(np.float32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb7747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Problem setup\n",
    "# -------------------------------\n",
    "\n",
    "def setup_training_world():\n",
    "    world = IcyGridWorld(size=5, p_slip=0.2)\n",
    "    reward = np.zeros(world.n_states, dtype=np.float32)\n",
    "    reward[-1] = 1.0\n",
    "    reward[8] = 0.65\n",
    "    terminal = np.array([world.n_states - 1], dtype=np.int64)\n",
    "    return world, reward, terminal\n",
    "\n",
    "\n",
    "def generate_expert_data(world, reward, terminal, discount=0.9, n_trajectories=200, seed=11, weighting_exp=50):\n",
    "    value = value_iteration(world.p_transition, reward, discount)\n",
    "    policy = stochastic_policy_from_value(world, value, weighting=lambda v: v ** weighting_exp)\n",
    "    start_states = np.array([0], dtype=np.int64)\n",
    "    return generate_trajectories(world, policy, start_states, terminal, n_trajectories, seed)\n",
    "\n",
    "\n",
    "world, expert_reward, terminal_states = setup_training_world()\n",
    "features = state_features(world)\n",
    "expert_trajectories = generate_expert_data(world, expert_reward, terminal_states)\n",
    "print(f\"World: {world.size}x{world.size}, trajectories: {len(expert_trajectories)}, feature dim: {features.shape[1]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6fcdca",
   "metadata": {},
   "source": [
    "Recall that we estimate the demonstration features via\n",
    "$$\\hat{\\mu}_E = \\frac{1}{N} \\sum_{i=1}^N \\sum_t \\phi(s_t^{(i)}).$$\n",
    "The helper just accumulates feature vectors along each valid trajectory and averages across the demos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f277aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_expectation_from_trajectories(features: np.ndarray, demos: Iterable[Trajectory]) -> np.ndarray:\n",
    "    totals = np.zeros(features.shape[1], dtype=np.float64)\n",
    "    n_trajectories = 0\n",
    "    for traj in demos:\n",
    "        visited = traj.state_sequence()\n",
    "        if visited.size == 0:\n",
    "            continue\n",
    "        # student code here\n",
    "        # TODO: sum the feature vectors for visited states and keep count of valid demos\n",
    "        raise NotImplementedError(\"TODO: fill in this block\")\n",
    "        # end student code\n",
    "    return totals / n_trajectories\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55e5cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_probabilities_from_trajectories(n_states: int, demos: Iterable[Trajectory]) -> np.ndarray:\n",
    "    counts = np.zeros(n_states, dtype=np.float64)\n",
    "    for traj in demos:\n",
    "        trans = traj.transitions()\n",
    "        if not trans:\n",
    "            continue\n",
    "        counts[trans[0].s_from] += 1.0\n",
    "    # student code here\n",
    "    # TODO: normalize the start-state histogram into a valid probability distribution\n",
    "    raise NotImplementedError(\"TODO: fill in this block\")\n",
    "    # end student code\n",
    "    return counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d70a0c",
   "metadata": {},
   "source": [
    "#### Backward partitions and expected visitation frequencies\n",
    "\n",
    "This function implements the MaxEnt IRL recursion:\n",
    "$$Z_{s,a} = e^{r(s)} \\sum_{s'} P(s'\\mid s,a) Z_{s'} , \\qquad Z_s = \\sum_a Z_{s,a}.$$\n",
    "Once the policy $\\pi(a\\mid s) = Z_{s,a} / Z_s$ is available, push visitation counts with\n",
    "$$d_{t+1}(s') = \\sum_{s,a} d_t(s)\\,\\pi(a\\mid s)\\,P(s'\\mid s,a).$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84deef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_expected_svf(p_transition: np.ndarray, p_initial: np.ndarray, terminal: Iterable[int], reward: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    n_states, _, n_actions = p_transition.shape\n",
    "    terminal = set(int(t) for t in terminal)\n",
    "\n",
    "    Zs = np.zeros(n_states, dtype=np.float64)\n",
    "    Zs[list(terminal)] = 1.0\n",
    "    exp_reward = np.exp(reward)\n",
    "\n",
    "    for _ in range(2 * n_states):\n",
    "        Za = np.zeros((n_states, n_actions), dtype=np.float64)\n",
    "        for s in range(n_states):\n",
    "            for a in range(n_actions):\n",
    "                # student code here\n",
    "                # TODO: compute the state-action partition value using the backward recursion\n",
    "                raise NotImplementedError(\"TODO: fill in this block\")\n",
    "                # end student code\n",
    "        new_Zs = Za.sum(axis=1)\n",
    "        if np.max(np.abs(new_Zs - Zs)) < eps:\n",
    "            Zs = new_Zs\n",
    "            break\n",
    "        Zs = new_Zs\n",
    "\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        policy = np.divide(Za, Zs[:, None], out=np.zeros_like(Za), where=Zs[:, None] > 0)\n",
    "\n",
    "    horizon = 2 * n_states\n",
    "    d = np.zeros((n_states, horizon), dtype=np.float64)\n",
    "    d[:, 0] = p_initial\n",
    "    for t in range(1, horizon):\n",
    "        for s_prev in range(n_states):\n",
    "            if s_prev in terminal:\n",
    "                continue\n",
    "            for a in range(n_actions):\n",
    "                prob = d[s_prev, t - 1] * policy[s_prev, a]\n",
    "                if prob == 0:\n",
    "                    continue\n",
    "                # student code here\n",
    "                # TODO: push visitation probability mass forward through the transition model\n",
    "                raise NotImplementedError(\"TODO: fill in this block\")\n",
    "                # end student code\n",
    "    return d.sum(axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4e78e2",
   "metadata": {},
   "source": [
    "#### Exponentiated-gradient MaxEnt IRL update\n",
    "\n",
    "At each iteration we compute the gradient\n",
    "$$\\nabla_\\omega = \\mu_E - \\mu_{\\pi_\\omega}$$\n",
    "and apply an exponentiated-gradient step\n",
    "$$\\omega \\leftarrow \\omega \\odot \\exp(\\alpha_t \\nabla_\\omega), \\qquad \\alpha_t = \\frac{\\alpha}{1 + t}.$$\n",
    "This keeps feature weights positive while shrinking the step size over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3a06d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxent_irl(p_transition: np.ndarray, features: np.ndarray, terminal: Iterable[int], demos: Iterable[Trajectory], lr: float = 0.2, eps: float = 1e-4, max_iter: int = 1000) -> np.ndarray:\n",
    "    demos = list(demos)\n",
    "    empirical = feature_expectation_from_trajectories(features, demos)\n",
    "    p_initial = initial_probabilities_from_trajectories(p_transition.shape[0], demos)\n",
    "\n",
    "    omega = np.ones(features.shape[1], dtype=np.float64)\n",
    "    step = 0\n",
    "    delta = np.inf\n",
    "    while delta > eps and step < max_iter:\n",
    "        omega_old = omega.copy()\n",
    "        reward = features @ omega\n",
    "        expected_svf = compute_expected_svf(p_transition, p_initial, terminal, reward)\n",
    "        model_features = features.T @ expected_svf\n",
    "        # student code here\n",
    "        # TODO: form the feature-expectation gradient and apply an exponentiated-gradient ascent step with a decaying learning rate\n",
    "        raise NotImplementedError(\"TODO: fill in this block\")\n",
    "        # end student code\n",
    "        delta = np.max(np.abs(omega - omega_old))\n",
    "        step += 1\n",
    "    learned_reward = features @ omega\n",
    "    return learned_reward\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5fce54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "\n",
    "def assert_close(name: str, actual, expected, atol=1e-6):\n",
    "    actual = np.asarray(actual, dtype=float)\n",
    "    expected = np.asarray(expected, dtype=float)\n",
    "    diff = float(np.max(np.abs(actual - expected)))\n",
    "    if diff > atol:\n",
    "        raise AssertionError(f\"{name} mismatch (max |Δ|={diff:.3e}). Expected {expected}, got {actual}\")\n",
    "    print(f\"[OK] {name}: max |Δ|={diff:.2e}\")\n",
    "\n",
    "\n",
    "def assert_probability_vector(name: str, vec):\n",
    "    total = float(np.sum(vec))\n",
    "    if not np.isclose(total, 1.0, atol=1e-6):\n",
    "        raise AssertionError(f\"{name} should sum to 1.0 but sums to {total:.6f}\")\n",
    "    if np.any(vec < -1e-8):\n",
    "        raise AssertionError(f\"{name} contains negative mass: {vec}\")\n",
    "    print(f\"[OK] {name}: sums to 1.0\")\n",
    "\n",
    "\n",
    "simple_features = np.eye(3)\n",
    "traj_a = Trajectory([Transition(0, 0, 1), Transition(1, 0, 2)])\n",
    "traj_b = Trajectory([Transition(0, 0, 0), Transition(0, 0, 1)])\n",
    "fe = feature_expectation_from_trajectories(simple_features, [traj_a, traj_b])\n",
    "assert_close(\"Feature expectations\", fe, np.array([1.5, 1.0, 0.5]))\n",
    "\n",
    "pi0 = initial_probabilities_from_trajectories(3, [traj_a, traj_b])\n",
    "assert_probability_vector(\"Initial-state histogram\", pi0)\n",
    "assert_close(\"Initial-state histogram entries\", pi0, np.array([1.0, 0.0, 0.0]))\n",
    "\n",
    "p_chain = np.zeros((3, 3, 1))\n",
    "p_chain[0, 1, 0] = 1.0\n",
    "p_chain[1, 2, 0] = 1.0\n",
    "p_chain[2, 2, 0] = 1.0\n",
    "svf = compute_expected_svf(p_chain, np.array([1.0, 0.0, 0.0]), [2], np.zeros(3))\n",
    "assert_close(\"Expected SVF sanity check\", svf, np.array([1.0, 1.0, 1.0]))\n",
    "\n",
    "learned_reward = maxent_irl(world.p_transition, features, terminal_states, expert_trajectories)\n",
    "p_initial_world = initial_probabilities_from_trajectories(world.n_states, expert_trajectories)\n",
    "svf_world = compute_expected_svf(world.p_transition, p_initial_world, terminal_states, learned_reward)\n",
    "empirical_features = feature_expectation_from_trajectories(features, expert_trajectories)\n",
    "model_features = features.T @ svf_world\n",
    "max_feature_diff = float(np.max(np.abs(model_features - empirical_features)))\n",
    "print(f\"Max feature expectation gap: {max_feature_diff:.4f}\")\n",
    "if max_feature_diff > 0.85:\n",
    "    raise AssertionError(\n",
    "        \"Learned reward does not yet match demonstration feature expectations closely enough.\"\n",
    "    )\n",
    "print(\"Section 1 tests passed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbe5227",
   "metadata": {},
   "source": [
    "#### Visualizing learned rewards and policies\n",
    "After training, we can visualize the learned reward function and the induced policy. These plots illustrate how well the IRL algorithm has captured the expert's behavior through the inferred reward structure. Your estimated rewards should have the correct top two high reward states, but expect some noise in the lower-value states due to limited demonstration data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ac783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_reward_comparison(world, expert_reward, learned_reward):\n",
    "    side = world.size\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    vmin = min(expert_reward.min(), learned_reward.min())\n",
    "    vmax = max(expert_reward.max(), learned_reward.max())\n",
    "    im0 = axes[0].imshow(expert_reward.reshape(side, side), cmap=\"viridis\", vmin=vmin, vmax=vmax)\n",
    "    axes[0].set_title(\"Expert Reward\")\n",
    "    axes[1].imshow(learned_reward.reshape(side, side), cmap=\"viridis\", vmin=vmin, vmax=vmax)\n",
    "    axes[1].set_title(\"Learned Reward\")\n",
    "    for ax in axes:\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    fig.colorbar(im0, ax=axes.ravel().tolist(), shrink=0.75)\n",
    "    plt.show()\n",
    "\n",
    "plot_reward_comparison(world, expert_reward, learned_reward)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a12e551",
   "metadata": {},
   "source": [
    "## 2. GRPO Fine-Tuning from Scratch [40 pts]\n",
    "\n",
    "1. Conceptual questions [10 pts]\n",
    "2. GRPO implementation [30 pts]\n",
    "\n",
    "Group Relative Policy Optimization (GRPO) extends PPO for RLHF by sampling multiple responses per prompt and using their mean reward as a control variate. This stabilizes policy updates when rewards come from evaluators or heuristics--exactly the setting for aligning language and multimodal models.\n",
    "\n",
    "**Suggested resources.**\n",
    "- Zhihong Shao et al., *DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models*, 2024 (introduces GRPO)\n",
    "- Hugging Face TRL blog, *GRPO: Group Relative Policy Optimization*\n",
    "- lsdefine/simple_GRPO GitHub repository (minimal open-source implementation)\n",
    "\n",
    "The remaining cells guide you through implementing the reward, advantage, and loss components so the GRPO training loop actually runs end-to-end.\n",
    "\n",
    "**GRPO recap (Shao et al., 2024).** For each prompt $i$ we draw a group of $m$ completions $\\{y_{i,j}\\}_{j=1}^m$ from the policy. Rewards $r_{i,j}$ are centered with the group mean $\\bar r_i = \\tfrac{1}{m}\\sum_j r_{i,j}$ to form advantages $A_{i,j} = r_{i,j} - \\bar r_i$. The policy update maximizes the PPO-style surrogate $\\min\\big(r_{i,j}A_{i,j}, \\operatorname{clip}(r_{i,j}, 1 \\pm \\epsilon)A_{i,j}\\big)$ while subtracting a KL penalty $\\beta\\,\\mathrm{KL}(\\pi_\\theta \\Vert \\pi_{\\text{ref}})$ to keep the fine-tuned model close to the frozen reference. This notebook isolates each mathematical piece so you can test it in isolation before chaining them together.\n",
    "\n",
    "**What you are implementing.** You will (a) code the structural reward checker, (b) normalize group advantages, (c) derive the GRPO loss from the log-prob ratios, and (d) plug everything into a gradient-accumulated training loop that mirrors Algorithm 1 in the paper. The surrounding evaluation cells verify that format adherence improves after RL fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bb158d",
   "metadata": {},
   "source": [
    "### 2.a Conceptual Questions [10 pts total; 2 pts each]\n",
    "\n",
    "1. Why does GRPO draw $G>1$ completions per prompt, and what is the prompt-level control variate used to center rewards?\n",
    "<font color=\"blue\">Your answer here.</font>\n",
    "\n",
    "2. A prompt yields rewards $(2, 1, -1)$ with group size $G=3$. Compute the centered advantages for each completion.\n",
    "<font color=\"blue\">Your answer here.</font>\n",
    "\n",
    "3. Midway through training you double $\\beta$ in the GRPO loss. Describe the effect this has on the update dynamics.\n",
    "<font color=\"blue\">Your answer here.</font>\n",
    "\n",
    "4. Starting from $r_t = \\exp(\\log p_\\theta - \\log p_{\\text{ref}})$, write the clipped surrogate used by GRPO and explain how the clipping interacts with the sign of the advantage.\n",
    "<font color=\"blue\">Your answer here.</font>\n",
    "\n",
    "5. Format rewards are dense but answer-correctness rewards are sparse. Propose a shaping scheme that keeps correctness learning signal meaningful and justify it.\n",
    "<font color=\"blue\">Your answer here.</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa14098",
   "metadata": {},
   "source": [
    "### 2.b GRPO Implementation [30 pts]\n",
    "\n",
    "Complete the reward shaping helpers, grouped-advantage computation, GRPO loss, and the short training/eval loop. Tests in Section 2 cover each of these components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9074abbc",
   "metadata": {},
   "source": [
    "### Section 2 implementation guide\n",
    "\n",
    "We now pivot to GRPO fine-tuning. The next block of cells loads libraries, defines the system & user prompts, constructs the reward function, and wires together helper utilities (log-prob extraction, grouped advantages, optimizer setup). Treat those as scaffolding so you can focus on the GRPO-specific TODOs and diagnostics. In particular, the device/import setup, dataset builder, model + LoRA configuration, and format/decoding helper cells are boilerplate—you do not need to edit them.\n",
    "\n",
    "Expect to run the provided GRPO loop on GPU; with the given reward shaping it can drive format adherence close to 100% after enough steps. Format adherence thresholds are tiered: 50% post-RL earns 2 pts, 70% earns 4 pts, and 90% earns the full 6 pts, so feel free to keep training until you hit the ceiling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fff39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e6cdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"You are a helpful assistant. First reason internally, then provide the user answer. \"\n",
    "    \"Respond using the template:\\n\"\n",
    "    \"<think>\\n\"\n",
    "    \"...\\n\"\n",
    "    \"</think>\\n\"\n",
    "    \"<answer>\\n\"\n",
    "    \"...\\n\"\n",
    "    \"</answer>\"\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class QAExample:\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "\n",
    "def build_dataset(num_examples: int = 64) -> Dataset:\n",
    "    records = []\n",
    "    for _ in range(num_examples):\n",
    "        a = random.randint(10, 99)\n",
    "        b = random.randint(1, 9)\n",
    "        question = f\"If you add {a} and {b}, what is the result?\"\n",
    "        answer = str(a + b)\n",
    "        records.append({\"question\": question, \"answer\": answer})\n",
    "    return Dataset.from_list(records)\n",
    "\n",
    "train_ds = build_dataset()\n",
    "train_ds[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc40dc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\")\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model = model.to(device)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c444fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "FORMAT_PATTERN = re.compile(r\"^<think>\\n[\\s\\S]*\\n</think>\\n<answer>\\n[\\s\\S]*\\n</answer>$\", re.MULTILINE)\n",
    "\n",
    "def format_reward(responses: List[str]) -> torch.Tensor:\n",
    "    scores = [1.0 if FORMAT_PATTERN.match(resp.strip()) else 0.0 for resp in responses]\n",
    "    return torch.tensor(scores, dtype=torch.float32, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "format-reward-sanity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks for format_reward\n",
    "valid_resp = \"<think>\\nreasoning goes here\\n</think>\\n<answer>\\nthe meaning of life is\\n</answer>\"\n",
    "invalid_resp = \"<think>missing closing tags\"\n",
    "scores = format_reward([valid_resp, invalid_resp])\n",
    "print('format_reward scores:', scores.cpu().tolist())\n",
    "assert scores[0].item() == 1.0\n",
    "assert scores[1].item() == 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39001330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question: str) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "prompts = [build_prompt(rec[\"question\"]) for rec in train_ds]\n",
    "prompts[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6c6a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def completion_log_probs(model, sequences: torch.Tensor, prompt_lengths: torch.Tensor) -> torch.Tensor:\n",
    "    attention_mask = (sequences != tokenizer.pad_token_id).long()\n",
    "    outputs = model(input_ids=sequences, attention_mask=attention_mask)\n",
    "    logits = outputs.logits[:, :-1, :]\n",
    "    target_ids = sequences[:, 1:]\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "    token_logps = torch.gather(log_probs, dim=-1, index=target_ids.unsqueeze(-1)).squeeze(-1)\n",
    "    cont_logps = []\n",
    "    for i in range(sequences.size(0)):\n",
    "        prompt_len = prompt_lengths[i].item()\n",
    "        total_len = attention_mask[i].sum().item()\n",
    "        start = max(prompt_len - 1, 0)\n",
    "        end = max(total_len - 1, start)\n",
    "        cont_logps.append(token_logps[i, start:end].sum())\n",
    "    return torch.stack(cont_logps)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_group(\n",
    "    model,\n",
    "    prompts: List[str],\n",
    "    num_generations: int = 2,\n",
    "    max_new_tokens: int = 128,\n",
    "    do_sample: bool = True,\n",
    "    temperature: float = 0.9,\n",
    "):\n",
    "    encoded = tokenizer(prompts, return_tensors=\"pt\", padding=True, add_special_tokens=False)\n",
    "    prompt_lengths = encoded[\"attention_mask\"].sum(dim=1)\n",
    "    encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "\n",
    "    gen_kwargs = {\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"do_sample\": do_sample,\n",
    "        \"num_return_sequences\": num_generations,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    }\n",
    "    if do_sample:\n",
    "        gen_kwargs[\"temperature\"] = temperature\n",
    "\n",
    "    outputs = model.generate(**encoded, **gen_kwargs)\n",
    "\n",
    "    batch_size = len(prompts)\n",
    "    sequences = outputs.view(batch_size, num_generations, -1)\n",
    "    seq_list = []\n",
    "    prompt_len_list = []\n",
    "    texts = []\n",
    "    for i in range(batch_size):\n",
    "        prompt_len = prompt_lengths[i].item()\n",
    "        for g in range(num_generations):\n",
    "            ids = sequences[i, g]\n",
    "            mask = (ids != tokenizer.pad_token_id).long()\n",
    "            seq_len = mask.sum().item()\n",
    "            seq_list.append(ids)\n",
    "            prompt_len_list.append(prompt_len)\n",
    "            text_tokens = ids[prompt_len:seq_len]\n",
    "            texts.append(tokenizer.decode(text_tokens, skip_special_tokens=True))\n",
    "    stacked = torch.stack(seq_list).to(device)\n",
    "    prompt_len_tensor = torch.tensor(prompt_len_list, device=device)\n",
    "    rewards = format_reward(texts)\n",
    "    return stacked, prompt_len_tensor, rewards, texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f42a9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_group_advantages(rewards: torch.Tensor, group_size: int) -> torch.Tensor:\n",
    "    if group_size <= 0:\n",
    "        raise ValueError(f\"group_size must be positive; got {group_size}\")\n",
    "    num_rewards = rewards.numel()\n",
    "    if num_rewards % group_size != 0:\n",
    "        raise ValueError(\n",
    "            f\"Reward tensor of length {num_rewards} is not divisible by group_size={group_size}\"\n",
    "        )\n",
    "    reshaped = rewards.view(-1, group_size)\n",
    "    mean = reshaped.mean(dim=1, keepdim=True)\n",
    "    std = reshaped.std(dim=1, keepdim=True, unbiased=False).clamp_min(1e-6)\n",
    "    normalized = (reshaped - mean) / std\n",
    "    return normalized.view(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1197018",
   "metadata": {},
   "source": [
    "#### GRPO clipped surrogate with KL anchor\n",
    "\n",
    "For each sampled sequence we form\n",
    "$$r_t = \\exp(\\log p_\\theta - \\log p_{\\text{ref}}), \\qquad L_{\\text{clip}} = \\min(r_t A_t, \\mathrm{clip}(r_t, 1\\pm\\epsilon) A_t).$$\n",
    "The full loss subtracts a KL penalty against the frozen reference policy:\n",
    "$$\\mathcal{L}_{\\text{GRPO}} = L_{\\text{clip}} - \\beta\\,\\mathrm{KL}(\\pi_\\theta\\,\\|\\,\\pi_{\\text{ref}}).$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c428c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grpo_sequence_loss(logp_new: torch.Tensor, logp_ref: torch.Tensor, advantages: torch.Tensor, beta: float, epsilon: float) -> torch.Tensor:\n",
    "    # student code here\n",
    "    # TODO: compute the clipped PPO-style surrogate and subtract the GRPO KL penalty term\n",
    "    raise NotImplementedError(\"TODO: fill in this block\")\n",
    "    # end student code\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba67280",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "beta = 0.05\n",
    "epsilon = 0.2\n",
    "group_size = 2\n",
    "\n",
    "\n",
    "def grpo_update(batch_prompts: List[str], grad_accum: int | None = None):\n",
    "    accum_steps = grad_accum if grad_accum is not None else 1\n",
    "    if accum_steps < 1:\n",
    "        raise ValueError(\"grad_accum must be >= 1\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    all_rewards: list[torch.Tensor] = []\n",
    "    all_losses: list[torch.Tensor] = []\n",
    "    collected_texts: list[str] = []\n",
    "\n",
    "    for _ in range(accum_steps):\n",
    "        sequences, prompt_lens, rewards, texts = sample_group(\n",
    "            model,\n",
    "            batch_prompts,\n",
    "            num_generations=group_size,\n",
    "        )\n",
    "\n",
    "        logp_new = completion_log_probs(model, sequences, prompt_lens)\n",
    "\n",
    "        adapters_supported = hasattr(model, \"disable_adapter\") and hasattr(model, \"enable_adapter\")\n",
    "        if adapters_supported:\n",
    "            model.disable_adapter()\n",
    "        with torch.no_grad():\n",
    "            logp_ref = completion_log_probs(model, sequences, prompt_lens)\n",
    "        if adapters_supported:\n",
    "            model.enable_adapter()\n",
    "\n",
    "        advantages = compute_group_advantages(rewards, group_size)\n",
    "        base_loss = grpo_sequence_loss(logp_new, logp_ref, advantages, beta=beta, epsilon=epsilon)\n",
    "        (base_loss / accum_steps).backward()\n",
    "\n",
    "        all_losses.append(base_loss.detach())\n",
    "        all_rewards.append(rewards.detach())\n",
    "        collected_texts.extend(texts)\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    reward_tensor = torch.cat(all_rewards)\n",
    "    reward_mean = reward_tensor.mean().item()\n",
    "    reward_std = reward_tensor.std().item() if reward_tensor.numel() > 1 else 0.0\n",
    "    loss_value = torch.stack(all_losses).mean().item()\n",
    "\n",
    "    return {\n",
    "        \"loss\": loss_value,\n",
    "        \"reward_mean\": reward_mean,\n",
    "        \"reward_std\": reward_std,\n",
    "        \"format_match_pct\": reward_mean * 100.0,\n",
    "        \"texts\": collected_texts,\n",
    "        \"optimizer_step\": True,\n",
    "        \"accumulated_microbatches\": accum_steps,\n",
    "        \"grad_accum_steps\": accum_steps,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1572e1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit tests for helpers\n",
    "\n",
    "def check_tensor_close(name: str, actual: torch.Tensor, expected: torch.Tensor, atol: float = 1e-6):\n",
    "    diff = torch.max(torch.abs(actual - expected)).item()\n",
    "    if diff > atol:\n",
    "        raise AssertionError(f\"{name} mismatch (max |Δ|={diff:.3e}). Expected {expected}, got {actual}\")\n",
    "    print(f\"[OK] {name}: max diff={diff:.2e}\")\n",
    "\n",
    "raw_rewards = torch.tensor([1.0, 0.0, 2.0, 4.0], device=device)\n",
    "adv = compute_group_advantages(raw_rewards, group_size=2)\n",
    "reshaped_adv = adv.view(2, 2)\n",
    "check_tensor_close(\"Group advantage mean\", reshaped_adv.mean(dim=1), torch.zeros(2, device=device))\n",
    "check_tensor_close(\"Group advantage std\", reshaped_adv.std(dim=1, unbiased=False), torch.ones(2, device=device))\n",
    "\n",
    "logp_new = torch.log(torch.tensor([0.6, 0.4, 0.7, 0.3], device=device))\n",
    "logp_ref = torch.log(torch.tensor([0.5, 0.5, 0.6, 0.4], device=device))\n",
    "manual_adv = torch.tensor([1.2, -0.4, 0.5, -1.0], device=device)\n",
    "loss = grpo_sequence_loss(logp_new, logp_ref, manual_adv, beta=0.1, epsilon=0.2)\n",
    "check_tensor_close(\"GRPO loss test\", loss, -0.2233469)\n",
    "\n",
    "logp_new_grad = torch.log(torch.tensor([0.55, 0.45, 0.65, 0.35], device=device))\n",
    "logp_new_grad.requires_grad_(True)\n",
    "loss_grad = grpo_sequence_loss(logp_new_grad, logp_ref, manual_adv, beta=0.05, epsilon=0.2)\n",
    "loss_grad.backward()\n",
    "if not torch.all(torch.isfinite(logp_new_grad.grad)):\n",
    "    raise AssertionError(\"Non-finite gradients detected in GRPO loss\")\n",
    "print(f\"[OK] GRPO loss backward pass: grad norm={logp_new_grad.grad.norm().item():.4f}\")\n",
    "\n",
    "print(\"Section 2 helper tests passed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "format-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_format_rate(\n",
    "    model,\n",
    "    dataset,\n",
    "    num_examples: int | None = 32,\n",
    "    num_generations: int = 1,\n",
    "    max_new_tokens: int = 128,\n",
    "    do_sample: bool = False,\n",
    "):\n",
    "    if num_examples is None:\n",
    "        subset = dataset\n",
    "    else:\n",
    "        size = min(num_examples, len(dataset))\n",
    "        subset = dataset.select(range(size))\n",
    "    effective_generations = num_generations\n",
    "    if not do_sample and num_generations != 1:\n",
    "        effective_generations = 1\n",
    "    prompts = [build_prompt(ex[\"question\"]) for ex in subset]\n",
    "    _, _, rewards, texts = sample_group(\n",
    "        model,\n",
    "        prompts,\n",
    "        num_generations=effective_generations,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "    )\n",
    "    total = rewards.numel()\n",
    "    matches = rewards.sum().item()\n",
    "    rate = (matches / total) * 100 if total else 0.0\n",
    "    print(f\"Format adherence: {rate:.1f}%\")\n",
    "    return rate, texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a05ffad",
   "metadata": {},
   "source": [
    "### Format evaluation and demo training loop\n",
    "\n",
    "After defining the helpers, we first measure baseline format adherence, then run a short GRPO training loop. Aim for at least 90% post-training adherence to receive full credit (the performance can climb near 100% after 25 steps if done correctly). Training should take approximately 30 minutes in Colab with a T4 GPU.\n",
    "\n",
    "Again, 50% post-RL earns 2 pts, 70% earns 4 pts, and 90% earns the full 6 pts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6a6f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = deque(maxlen=10)\n",
    "eval_subset = train_ds.select(range(min(32, len(train_ds))))\n",
    "print(\"Baseline format adherence (pre-RL):\")\n",
    "baseline_rate, _ = evaluate_format_rate(\n",
    "    model,\n",
    "    eval_subset,\n",
    "    num_examples=None,\n",
    "    num_generations=group_size,\n",
    "    do_sample=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a036a683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "for step in range(25):\n",
    "    batch = train_ds.shuffle(seed=step).select(range(4))  # batch size 4\n",
    "    prompts_batch = [build_prompt(ex[\"question\"]) for ex in batch]\n",
    "    stats = grpo_update(prompts_batch, grad_accum=8)  # simulate batch size 4x8=32\n",
    "    history.append(stats[\"reward_mean\"])\n",
    "    print(\n",
    "        f\"Step {step + 1:02d} | reward_mean={stats['reward_mean']:.3f} | format adherence={stats['format_match_pct']:.1f}% | time={(time.time() - start_time):.1f}s\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3827bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Format adherence after RL:\")\n",
    "post_rate, _ = evaluate_format_rate(\n",
    "    model,\n",
    "    eval_subset,\n",
    "    num_examples=None,\n",
    "    num_generations=group_size,\n",
    "    do_sample=False,\n",
    ")\n",
    "print(f\"Baseline: {baseline_rate:.1f}% | Post-RL: {post_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae9805d",
   "metadata": {},
   "source": [
    "## Submission Checklist \n",
    "\n",
    "- All answers filled.\n",
    "- Section 1 tests succeed; visualization optional.\n",
    "- Section 2 helper tests succeed; optional demo loop left commented.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bc76d1",
   "metadata": {},
   "source": [
    "### Chain-of-Thought Playground Example\n",
    "\n",
    "Use this cell to poke the fine-tuned model with your own prompts. We start with a simple 4-digit addition problem so you can see the expected <think>/<answer> chain of thought. Edit `example_question` and re-run as you like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4342a3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_question = \"What's 4821 + 7394?\"\n",
    "example_prompt = build_prompt(example_question)\n",
    "print(\"Prompt:\", example_question)\n",
    "\n",
    "inputs = tokenizer(example_prompt, return_tensors=\"pt\", padding=True, add_special_tokens=False).to(device)\n",
    "input_len = inputs[\"input_ids\"].shape[1]\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "new_tokens = output_ids[0, input_len:]\n",
    "completion = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "print(\"Model completion:\")\n",
    "print(completion.strip())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl-hw4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

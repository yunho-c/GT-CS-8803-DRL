{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88af3281",
   "metadata": {},
   "source": [
    "# HW 4 - Inverse RL & GRPO Fine-Tuning\n",
    "\n",
    "Reward modeling is the glue between human supervision and scalable RL systems. Classical inverse reinforcement learning (IRL) lets us infer reward functions directly from expert demonstrations, and that same idea underpins modern training pipelines (r.g. RLHF, RLAIF, Constitutional AI) where we fit reward models or preference models for large language models. Even though today's top AI labs rely on pairwise/ranked preference data or verifiable rewards rather than tabular MaxEnt IRL, the maximum entropy view is a foundational recipe: it teaches us to match feature expectations while keeping the trajectory distribution as high-entropy (i.e., as uncertain) as possible away from labeled data, which is a philosophy we still enforce when training modern reward models.\n",
    "\n",
    "In **Section 1**, we will apply Maximum Entropy Inverse Reinforcement Learning (MaxEnt IRL) algorithm from [Ziebart et al., 2008](https://cdn.aaai.org/AAAI/2008/AAAI08-227.pdf) to recover rewards from demonstrations.\n",
    "\n",
    "In **Section 2**, we will fine-tune a QLoRA-adapted large language model with the Group Relative Policy Optimization (GRPO) algorithm from [Shao et al., 2024](https://arxiv.org/abs/2402.03300) so the LLM outputs follow structured reasoning formats.\n",
    "\n",
    "\n",
    "**Runtime requirement.** This notebook assumes access to a GPU (MaxEnt IRL visualization is light, but GRPO + QLoRA fine-tuning will not run on CPU). If you are using Google Colab, open `Runtime → Change runtime type` and set the hardware accelerator to `GPU`, then restart the runtime before continuing.\n",
    "\n",
    "To conserve GPU hours, we recommend leaving the notebook on a CPU runtime while you work through Section 1 and the early GRPO setup cells. Switch to a GPU only once you reach the GRPO training section where accelerated generation becomes necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "660fc4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/homebrew/lib/python3.13/site-packages (4.57.0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: datasets in /opt/homebrew/lib/python3.13/site-packages (3.5.0)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: accelerate in /opt/homebrew/lib/python3.13/site-packages (1.6.0)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: peft in /opt/homebrew/lib/python3.13/site-packages (0.15.1)\n",
      "Collecting peft\n",
      "  Downloading peft-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting math-verify\n",
      "  Downloading math_verify-0.8.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.13/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/homebrew/lib/python3.13/site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.13/site-packages (from transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.13/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.13/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/homebrew/lib/python3.13/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/homebrew/lib/python3.13/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/homebrew/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-22.0.0-cp313-cp313-macosx_12_0_arm64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/homebrew/lib/python3.13/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.13/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/homebrew/lib/python3.13/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /opt/homebrew/lib/python3.13/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /opt/homebrew/lib/python3.13/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/homebrew/lib/python3.13/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.11.16)\n",
      "Requirement already satisfied: anyio in /opt/homebrew/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (1.0.7)\n",
      "Requirement already satisfied: idna in /opt/homebrew/lib/python3.13/site-packages (from httpx<1.0.0->datasets) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/homebrew/lib/python3.13/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.14.0)\n",
      "Requirement already satisfied: psutil in /opt/homebrew/lib/python3.13/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/homebrew/lib/python3.13/site-packages (from accelerate) (2.8.0)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/lib/python3.13/site-packages (from bitsandbytes) (1.16.1)\n",
      "Collecting latex2sympy2_extended==1.10.2 (from math-verify)\n",
      "  Downloading latex2sympy2_extended-1.10.2-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.13/site-packages (from latex2sympy2_extended==1.10.2->math-verify) (1.14.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime<=4.13.2,>=4.9.3 in /opt/homebrew/lib/python3.13/site-packages (from latex2sympy2_extended==1.10.2->math-verify) (4.9.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/homebrew/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/homebrew/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/homebrew/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.13/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.13/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (69.5.1)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.13/site-packages (from torch>=2.0.0->accelerate) (3.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/lib/python3.13/site-packages (from sympy->latex2sympy2_extended==1.10.2->math-verify) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/homebrew/lib/python3.13/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.13/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/lib/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.13/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/lib/python3.13/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
      "Downloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
      "Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.18.0-py3-none-any.whl (556 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m556.4/556.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading math_verify-0.8.0-py3-none-any.whl (29 kB)\n",
      "Downloading latex2sympy2_extended-1.10.2-py3-none-any.whl (207 kB)\n",
      "Downloading pyarrow-22.0.0-cp313-cp313-macosx_12_0_arm64.whl (34.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.2/34.2 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow, latex2sympy2_extended, bitsandbytes, math-verify, accelerate, transformers, datasets, peft\n",
      "\u001b[2K  Attempting uninstall: pyarrow\n",
      "\u001b[2K    Found existing installation: pyarrow 19.0.1\n",
      "\u001b[2K    Uninstalling pyarrow-19.0.1:\n",
      "\u001b[2K      Successfully uninstalled pyarrow-19.0.1\n",
      "\u001b[2K  Attempting uninstall: accelerate90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/8\u001b[0m [bitsandbytes]extended]\n",
      "\u001b[2K    Found existing installation: accelerate 1.6.0━━━━━━━━━━━━━\u001b[0m \u001b[32m2/8\u001b[0m [bitsandbytes]\n",
      "\u001b[2K    Uninstalling accelerate-1.6.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/8\u001b[0m [bitsandbytes]\n",
      "\u001b[2K      Successfully uninstalled accelerate-1.6.0━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/8\u001b[0m [bitsandbytes]\n",
      "\u001b[2K  Attempting uninstall: transformersm╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/8\u001b[0m [accelerate]\n",
      "\u001b[2K    Found existing installation: transformers 4.57.0━━━━━━━━━━\u001b[0m \u001b[32m4/8\u001b[0m [accelerate]\n",
      "\u001b[2K    Uninstalling transformers-4.57.0:\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/8\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.57.0━━━━━━━━━━━━\u001b[0m \u001b[32m5/8\u001b[0m [transformers]\n",
      "\u001b[2K  Attempting uninstall: datasets━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/8\u001b[0m [transformers]\n",
      "\u001b[2K    Found existing installation: datasets 3.5.0m━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/8\u001b[0m [transformers]\n",
      "\u001b[2K    Uninstalling datasets-3.5.0:m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/8\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled datasets-3.5.090m━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/8\u001b[0m [transformers]\n",
      "\u001b[2K  Attempting uninstall: peft━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m6/8\u001b[0m [datasets]\n",
      "\u001b[2K    Found existing installation: peft 0.15.1\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m6/8\u001b[0m [datasets]\n",
      "\u001b[2K    Uninstalling peft-0.15.1:━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m6/8\u001b[0m [datasets]\n",
      "\u001b[2K      Successfully uninstalled peft-0.15.1m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m6/8\u001b[0m [datasets]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [peft][32m7/8\u001b[0m [peft]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.11.0 bitsandbytes-0.42.0 datasets-4.4.1 latex2sympy2_extended-1.10.2 math-verify-0.8.0 peft-0.18.0 pyarrow-22.0.0 transformers-4.57.1\n"
     ]
    }
   ],
   "source": [
    "# If running on Colab (recommended)\n",
    "!pip install -U transformers datasets accelerate bitsandbytes peft math-verify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b937b9",
   "metadata": {},
   "source": [
    "## 1. Maximum Entropy IRL [40 pts]\n",
    "\n",
    "1. Conceptual questions [12 pts]\n",
    "2. MaxEnt IRL implementation [28 pts]\n",
    "\n",
    "Maximum Entropy IRL (Ziebart et al., 2008) infers a reward function that explains expert demonstrations while remaining as uncertain as possible beyond the observed behavior. By matching **feature** expectations under a stochastic policy, we recover rewards that generalize better than directly cloning actions.\n",
    "\n",
    "**Suggested resources.**\n",
    "- B. D. Ziebart et al., *Maximum Entropy Inverse Reinforcement Learning*, AAAI 2008\n",
    "- B. D. Ziebart, *Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy*, PhD thesis, 2010\n",
    "- S. Levine, *Reinforcement Learning and Control as Probabilistic Inference*, NeurIPS 2018 tutorial\n",
    "\n",
    "The exercises that follow rebuild the MaxEnt IRL pipeline from demonstrations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2670749",
   "metadata": {},
   "source": [
    "### The IRL objective\n",
    "\n",
    "We work with a Markov decision process $(S, A, T, R, \\gamma)$ where transitions $T(s_t, a_t, s_{t+1})$ and the discount $\\gamma$ are known while the per-state reward $R(s)$ is not. Reinforcement learning searches for a policy $\\pi$ that maximizes expected discounted return, but in inverse RL we only watch demonstrations $\\mathcal{D} = \\{\\tau_i\\}$ produced by an expert policy $\\pi^E$. Each trajectory $\\tau = ((s_0, a_0), \\ldots, s_T)$ is summarized through hand-crafted features $\\phi: S \\to \\mathbb{R}^d$, and we assume a linear reward $R_\\omega(s) = \\omega^\\top \\phi(s)$. The goal is to recover parameters $\\omega$ whose induced behavior explains the demonstrations as well as the original expert policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60f195a",
   "metadata": {},
   "source": [
    "### Principle of Maximum Entropy\n",
    "\n",
    "Feature-expectation matching requires the learner to visit features just as often as the expert does,\n",
    "$$\n",
    "    \\mathbb{E}_{\\pi^L}[\\phi(\\tau)] = \\mathbb{E}_{\\pi^E}[\\phi(\\tau)],\n",
    "$$\n",
    "where the learner's policy $\\pi^L$ emerges from the trajectory distribution $p(\\tau)$. This constraint alone leaves infinitely many rewards that can explain $\\mathcal{D}$. Following Jaynes (1957) and Ziebart et al. (2008), we pick the solution with maximum entropy so that we add no extra bias beyond the data we observed. Intuitively, higher-entropy distributions encode fewer additional assumptions—**if two policies both satisfy the feature constraints, we prefer the one whose trajectories remain as uncertain as possible**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e6fd9a",
   "metadata": {},
   "source": [
    "### Trajectory distributions, gradients, and state visitation frequencies\n",
    "\n",
    "Applying Lagrange multipliers yields a trajectory distribution of the form\n",
    "$$\n",
    "    p(\\tau \\mid \\omega) = \\frac{1}{Z(\\omega)} \\exp(\\omega^\\top \\phi(\\tau)) \\prod_{t} p(s_{t+1} \\mid s_t, a_t),\n",
    "$$\n",
    "with partition function $Z(\\omega)$ ensuring normalization. Maximizing the log-likelihood of the demonstrations leads to the gradient\n",
    "$$\n",
    "    \\nabla_\\omega \\mathcal{L}(\\omega) = \\mu_E - \\sum_s D_s\\, \\phi(s),\n",
    "$$\n",
    "where $\\mu_E$ is the empirical feature expectation and $D_s$ is the time-aggregated expected state-visitation frequency (sum over all time steps) under the current reward. In the forward recursion below we will write $d_t(s)$ for the per-time-step visitation frequency at horizon index $t$, so that $D_s = \\sum_t d_t(s)$. Computing $D_s$ requires a backward pass for the partition functions ($Z_{s,a}$ and $Z_s$ provide the local action probabilities $\\pi(a \\mid s) = Z_{s,a} / Z_s$) followed by a forward rollout that propagates visitation counts from the initial-state distribution. These are the exact steps implemented below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50651d83",
   "metadata": {},
   "source": [
    "### 1.a Conceptual Questions [12 pts total; 3 pts each]\n",
    "\n",
    "1. Two demonstrations $\\tau_1$ and $\\tau_2$ accumulate feature sums $F_1$ and $F_2$. Under the MaxEnt IRL distribution, write the log-odds $\\log p(\\tau_1 \\mid \\omega) - \\log p(\\tau_2 \\mid \\omega)$ and state when the odds become zero.\n",
    "\n",
    "<font color=\"blue\">\n",
    "Given\n",
    "\n",
    "$$p(\\tau \\mid \\omega) = \\frac{1}{Z(\\omega)} \\exp(\\omega^\\intercal \\phi(\\tau)) \\prod_t p(s_{t+1} \\mid s_t, a_t)$$\n",
    "\n",
    "and thus\n",
    "\n",
    "$$p(\\tau \\mid \\omega) \\propto \\exp(\\omega^\\intercal \\phi(\\tau))$$\n",
    "\n",
    "We seek to find\n",
    "\n",
    "$$\\log p(\\tau_1 \\mid \\omega) - \\log p(\\tau_2 \\mid \\omega)$$\n",
    "\n",
    "$$= \\log(\\frac{\\frac{1}{Z(\\omega)} \\exp(\\omega^\\intercal \\phi(\\tau_1)) \\prod_t p(s_{t+1} \\mid s_t, a_t)}{\\frac{1}{Z(\\omega)} \\exp(\\omega^\\intercal \\phi(\\tau_2)) \\prod_t p(s_{t+1} \\mid s_t, a_t)})$$\n",
    "\n",
    "All factors, except $\\exp(\\omega^\\intercal \\phi(\\tau))$, cancel out—leaving\n",
    "\n",
    "$$= \\log(\\frac{\\exp(\\omega^\\intercal \\phi(\\tau_1))}{\\exp(\\omega^\\intercal \\phi(\\tau_2))})$$\n",
    "\n",
    "$$= \\log(\\exp(\\omega^\\intercal \\phi(\\tau_1))) - \\log(\\exp(\\omega^\\intercal \\phi(\\tau_2)))$$\n",
    "\n",
    "$$= \\omega^\\intercal \\phi(\\tau_1) - \\omega^\\intercal \\phi(\\tau_2)$$\n",
    "\n",
    "$$= \\omega^\\intercal (\\phi(\\tau_1) - \\phi(\\tau_2))$$\n",
    "\n",
    "$$= \\omega^\\intercal (F_1 - F_2)$$\n",
    "\n",
    "The odd becomes zero when $F_1 = F_2$.\n",
    "</font>\n",
    "\n",
    "2. Explain what goes wrong if you try to roll out expected visitation frequencies before finishing the backward partition pass.\n",
    "\n",
    "<font color=\"blue\">\n",
    "The backward partition pass serves the role of computing the value and Q-value functions (via dynamic programming), so that a downstream policy can be constructed from them. \n",
    "\n",
    "If the backward partition pass is not finished before the forward pass, the value and Q-value functions will be incorrect, and thus the forward pass (policy inference) will not correctly represent the current reward function (that is being learned). Thus, the gradients, loss, and optimization will update (feature importance) weights $\\omega$ in a way that probably does not help obtain a better reward function. \n",
    "</font>\n",
    "\n",
    "3. Provide the forward recursion for the expected visitation frequencies $d_{t+1}(s')$ **and** state how you prevent terminal states from re-emitting probability mass.\n",
    "\n",
    "The forward recursion of expected visitation frequencies:\n",
    "\n",
    "> $$d_{t+1}(s') = \\sum_s d_t(s) \\sum_a \\pi (a \\mid s) T(s, a, s')$$\n",
    "\n",
    "> `NOTE`: Even though it's called \"recursion\", it doesn't involve a function calling itself... it just means it's rolling out a past-dependent value sequentially w.r.t time (more like a normal for loop to programming peeps).\n",
    "\n",
    "> The terminal states must not emit probability mass to subsequent (inner) iterations of the forward pass, since, well, they're terminal. However, they should still *accumulate* probability (i.e., allow other states to enter into the terminal states).\n",
    "\n",
    "> There are multiple ways to implement this in practice: \n",
    "> 1) Skip them in sum over s ($\\sum_s d_t(s) \\to \\sum_s d_t(s) \\: \\text{if} \\: s \\neq s_{\\text{terminal}}$)\n",
    "> 2) Set $\\pi(a \\mid s_{\\text{terminal}}) = 0$ for all $a$ (prevent outgoing actions)\n",
    "> 3) Set $T(s_\\text{terminal},a,s') = 0$ for $s' \\neq s_{\\text{terminal}}$.\n",
    "\n",
    "4. Name one numerical-stability technique for the partition or visitation computations and briefly describe how it mitigates underflow/overflow.\n",
    "\n",
    "> The `log-sum-exp` trick is used in the backward partition (soft value iteration) pass. \n",
    "\n",
    "> When computing $\\log \\sum_a \\exp(Q(s,a))$, directly computing exponents of large Q values can result in overflow errors, while doing so for small Q values can result in underflow errors (truncation to zero). \n",
    "\n",
    "> The `log-sum-exp` trick,\n",
    "\n",
    "> $$\\log \\sum_a \\exp(Q_a) = m + \\log \\sum_a \\exp(Q_a - m)$$\n",
    "\n",
    "> where $m = \\max_a(Q_a)$,\n",
    "\n",
    "> essentially limits the dynamic range of values that go inside the exponential function into a numerically safe range, avoiding over/underflows. \n",
    "\n",
    "> This is especially important when there are many possible combinations of states and actions, because 1) there is a higher likelihood of a large Q value existing, and 2) the sum of exponents can grow to larger values (since there are more numbers to sum up). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68269765",
   "metadata": {},
   "source": [
    "### 1.b MaxEnt IRL Implementation [28 pts]\n",
    "\n",
    "Implement the four helpers below (feature expectations, initial-state distribution, expected state visitation frequencies, and the exponentiated-gradient solver). Section 1 unit tests check them in the same order.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb1a9d4",
   "metadata": {},
   "source": [
    "### Section 1 implementation roadmap\n",
    "\n",
    "The next three code cells are boilerplate: they define the GridWorld dynamics, trajectory containers, and data-generation utilities shared by this notebook. Skim them for context, but there is no TODO or grading logic inside.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f73ffbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from itertools import product\n",
    "from typing import Callable, Iterable, List\n",
    "\n",
    "# -------------------------------\n",
    "# Grid world dynamics\n",
    "# -------------------------------\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self, size: int):\n",
    "        self.size = size\n",
    "        self.actions = [(1, 0), (-1, 0), (0, 1), (0, -1)]\n",
    "        self.n_actions = len(self.actions)\n",
    "        self.n_states = size * size\n",
    "        self.p_transition = self._build_transition_table()\n",
    "\n",
    "    def state_index_to_point(self, state: int):\n",
    "        return state % self.size, state // self.size\n",
    "\n",
    "    def state_point_to_index(self, point):\n",
    "        x, y = point\n",
    "        return y * self.size + x\n",
    "\n",
    "    def _clip_point(self, point):\n",
    "        x, y = point\n",
    "        x = min(max(x, 0), self.size - 1)\n",
    "        y = min(max(y, 0), self.size - 1)\n",
    "        return x, y\n",
    "\n",
    "    def _step_intended(self, state: int, action: int):\n",
    "        x, y = self.state_index_to_point(state)\n",
    "        dx, dy = self.actions[action]\n",
    "        return self.state_point_to_index(self._clip_point((x + dx, y + dy)))\n",
    "\n",
    "    def _build_transition_table(self):\n",
    "        table = np.zeros((self.n_states, self.n_states, self.n_actions), dtype=np.float32)\n",
    "        for s_from, s_to, a in product(range(self.n_states), range(self.n_states), range(self.n_actions)):\n",
    "            table[s_from, s_to, a] = self._transition_prob(s_from, s_to, a)\n",
    "        return table\n",
    "\n",
    "    def _transition_prob(self, s_from, s_to, a):\n",
    "        fx, fy = self.state_index_to_point(s_from)\n",
    "        tx, ty = self.state_index_to_point(s_to)\n",
    "        ax, ay = self.actions[a]\n",
    "\n",
    "        # deterministic transition defined by action\n",
    "        if fx + ax == tx and fy + ay == ty:\n",
    "            return 1.0\n",
    "\n",
    "        # we can stay at the same state if we would move over an edge\n",
    "        if fx == tx and fy == ty:\n",
    "            if not 0 <= fx + ax < self.size or not 0 <= fy + ay < self.size:\n",
    "                return 1.0\n",
    "\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "class IcyGridWorld(GridWorld):\n",
    "    def __init__(self, size: int, p_slip: float):\n",
    "        self.p_slip = p_slip\n",
    "        super().__init__(size)\n",
    "\n",
    "    def _transition_prob(self, s_from, s_to, a):\n",
    "        fx, fy = self.state_index_to_point(s_from)\n",
    "        tx, ty = self.state_index_to_point(s_to)\n",
    "        ax, ay = self.actions[a]\n",
    "\n",
    "        # intended transition defined by action\n",
    "        if fx + ax == tx and fy + ay == ty:\n",
    "            return 1.0 - self.p_slip + self.p_slip / self.n_actions\n",
    "\n",
    "        # we can slip to all neighboring states\n",
    "        if abs(fx - tx) + abs(fy - ty) == 1:\n",
    "            return self.p_slip / self.n_actions\n",
    "\n",
    "        # we can stay at the same state if we would move over an edge\n",
    "        if fx == tx and fy == ty:\n",
    "            if not 0 <= fx + ax < self.size or not 0 <= fy + ay < self.size:\n",
    "                if not 0 < fx < self.size - 1 and not 0 < fy < self.size - 1:\n",
    "                    return 1.0 - self.p_slip + 2.0 * self.p_slip / self.n_actions\n",
    "                return 1.0 - self.p_slip + self.p_slip / self.n_actions\n",
    "\n",
    "            if not 0 < fx < self.size - 1 and not 0 < fy < self.size - 1:\n",
    "                return 2.0 * self.p_slip / self.n_actions\n",
    "            if not 0 < fx < self.size - 1 or not 0 < fy < self.size - 1:\n",
    "                return self.p_slip / self.n_actions\n",
    "            return 0.0\n",
    "\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def state_features(world: GridWorld) -> np.ndarray:\n",
    "    return np.eye(world.n_states, dtype=np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf4e514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Trajectories and solvers\n",
    "# -------------------------------\n",
    "\n",
    "@dataclass\n",
    "class Transition:\n",
    "    s_from: int\n",
    "    action: int\n",
    "    s_to: int\n",
    "\n",
    "\n",
    "class Trajectory:\n",
    "    def __init__(self, transitions: List[Transition]):\n",
    "        self._transitions = transitions\n",
    "\n",
    "    def transitions(self) -> List[Transition]:\n",
    "        return self._transitions\n",
    "\n",
    "    def state_sequence(self) -> np.ndarray:\n",
    "        states = [tr.s_from for tr in self._transitions]\n",
    "        if self._transitions:\n",
    "            states.append(self._transitions[-1].s_to)\n",
    "        return np.asarray(states, dtype=np.int64)\n",
    "\n",
    "\n",
    "def generate_trajectory(world: GridWorld, policy_logits: np.ndarray, start: int, terminal: Iterable[int], rng: np.random.Generator) -> Trajectory:\n",
    "    state = start\n",
    "    transitions = []\n",
    "    while state not in terminal:\n",
    "        probs = policy_logits[state]\n",
    "        action = rng.choice(world.n_actions, p=probs)\n",
    "        next_state = rng.choice(world.n_states, p=world.p_transition[state, :, action])\n",
    "        transitions.append(Transition(state, action, next_state))\n",
    "        state = next_state\n",
    "    return Trajectory(transitions)\n",
    "\n",
    "\n",
    "def generate_trajectories(world: GridWorld, policy_logits: np.ndarray, start_states: np.ndarray, terminal: Iterable[int], n_trajectories: int, seed: int = 0) -> List[Trajectory]:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    demos = []\n",
    "    for _ in range(n_trajectories):\n",
    "        s0 = rng.choice(start_states)\n",
    "        demos.append(generate_trajectory(world, policy_logits, s0, terminal, rng))\n",
    "    return demos\n",
    "\n",
    "\n",
    "def value_iteration(p_transition: np.ndarray, reward: np.ndarray, discount: float, eps: float = 1e-4) -> np.ndarray:\n",
    "    n_states, _, n_actions = p_transition.shape\n",
    "    v = np.zeros(n_states, dtype=np.float32)\n",
    "    delta = np.inf\n",
    "    while delta > eps:\n",
    "        v_old = v.copy()\n",
    "        q = np.zeros((n_states, n_actions), dtype=np.float32)\n",
    "        for a in range(n_actions):\n",
    "            q[:, a] = reward + discount * p_transition[:, :, a] @ v\n",
    "        v = q.max(axis=1)\n",
    "        delta = np.max(np.abs(v - v_old))\n",
    "    return v\n",
    "\n",
    "\n",
    "def stochastic_policy_from_value(world: GridWorld, value: np.ndarray, weighting: Callable[[float], float] = lambda x: x) -> np.ndarray:\n",
    "    softened = np.exp(value.astype(np.float64))\n",
    "    prefs = np.zeros((world.n_states, world.n_actions), dtype=np.float64)\n",
    "    for s in range(world.n_states):\n",
    "        for a in range(world.n_actions):\n",
    "            intended = world._step_intended(s, a)\n",
    "            prefs[s, a] = weighting(softened[intended])\n",
    "    row_sums = prefs.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0.0] = 1.0\n",
    "    return (prefs / row_sums).astype(np.float32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edb7747e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World: 5x5, trajectories: 200, feature dim: 25\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Problem setup\n",
    "# -------------------------------\n",
    "\n",
    "def setup_training_world():\n",
    "    world = IcyGridWorld(size=5, p_slip=0.2)\n",
    "    reward = np.zeros(world.n_states, dtype=np.float32)\n",
    "    reward[-1] = 1.0\n",
    "    reward[8] = 0.65\n",
    "    terminal = np.array([world.n_states - 1], dtype=np.int64)\n",
    "    return world, reward, terminal\n",
    "\n",
    "\n",
    "def generate_expert_data(world, reward, terminal, discount=0.9, n_trajectories=200, seed=11, weighting_exp=50):\n",
    "    value = value_iteration(world.p_transition, reward, discount)\n",
    "    policy = stochastic_policy_from_value(world, value, weighting=lambda v: v ** weighting_exp)\n",
    "    start_states = np.array([0], dtype=np.int64)\n",
    "    return generate_trajectories(world, policy, start_states, terminal, n_trajectories, seed)\n",
    "\n",
    "\n",
    "world, expert_reward, terminal_states = setup_training_world()\n",
    "features = state_features(world)\n",
    "expert_trajectories = generate_expert_data(world, expert_reward, terminal_states)\n",
    "print(f\"World: {world.size}x{world.size}, trajectories: {len(expert_trajectories)}, feature dim: {features.shape[1]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6fcdca",
   "metadata": {},
   "source": [
    "Recall that we estimate the demonstration features via\n",
    "$$\\hat{\\mu}_E = \\frac{1}{N} \\sum_{i=1}^N \\sum_t \\phi(s_t^{(i)}).$$\n",
    "The helper just accumulates feature vectors along each valid trajectory and averages across the demos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72f277aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_expectation_from_trajectories(features: np.ndarray, demos: Iterable[Trajectory]) -> np.ndarray:\n",
    "    totals = np.zeros(features.shape[1], dtype=np.float64)\n",
    "    n_trajectories = 0\n",
    "    for traj in demos:\n",
    "        visited = traj.state_sequence()\n",
    "        if visited.size == 0:\n",
    "            continue\n",
    "        # student code here\n",
    "        for state in traj.state_sequence():\n",
    "            totals += features[state]\n",
    "        n_trajectories += 1\n",
    "        # end student code\n",
    "    return totals / n_trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b55e5cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_probabilities_from_trajectories(n_states: int, demos: Iterable[Trajectory]) -> np.ndarray:\n",
    "    counts = np.zeros(n_states, dtype=np.float64)\n",
    "    for traj in demos:\n",
    "        trans = traj.transitions()\n",
    "        if not trans:\n",
    "            continue\n",
    "        counts[trans[0].s_from] += 1.0\n",
    "    # student code here\n",
    "    counts = counts / sum(counts)\n",
    "    # end student code\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d70a0c",
   "metadata": {},
   "source": [
    "#### Backward partitions and expected visitation frequencies\n",
    "\n",
    "This function implements the MaxEnt IRL recursion:\n",
    "$$Z_{s,a} = e^{r(s)} \\sum_{s'} P(s'\\mid s,a) Z_{s'} , \\qquad Z_s = \\sum_a Z_{s,a}.$$\n",
    "Once the policy $\\pi(a\\mid s) = Z_{s,a} / Z_s$ is available, push visitation counts with\n",
    "$$d_{t+1}(s') = \\sum_{s,a} d_t(s)\\,\\pi(a\\mid s)\\,P(s'\\mid s,a).$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84deef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_expected_svf(p_transition: np.ndarray, p_initial: np.ndarray, terminal: Iterable[int], reward: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    n_states, _, n_actions = p_transition.shape  # (s_from, s_to, a)\n",
    "    terminal = set(int(t) for t in terminal)\n",
    "\n",
    "    Zs = np.zeros(n_states, dtype=np.float64)\n",
    "    Zs[list(terminal)] = 1.0\n",
    "    exp_reward = np.exp(reward)\n",
    "\n",
    "    for _ in range(2 * n_states):\n",
    "        Za = np.zeros((n_states, n_actions), dtype=np.float64)\n",
    "        for s in range(n_states):\n",
    "            for a in range(n_actions):\n",
    "                # student code here\n",
    "                # TODO: compute the state-action partition value using the backward recursion\n",
    "                Za[s, a] = exp_reward[s] * sum(p_transition[s, :, a] * Zs)  # ??? # use log-sum-exp?\n",
    "                # end student code\n",
    "        new_Zs = Za.sum(axis=1)\n",
    "        if np.max(np.abs(new_Zs - Zs)) < eps:  # check for convergence and stop early\n",
    "            Zs = new_Zs\n",
    "            break\n",
    "        Zs = new_Zs\n",
    "\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        policy = np.divide(Za, Zs[:, None], out=np.zeros_like(Za), where=Zs[:, None] > 0)\n",
    "\n",
    "    horizon = 2 * n_states\n",
    "    d = np.zeros((n_states, horizon), dtype=np.float64)\n",
    "    d[:, 0] = p_initial\n",
    "    for t in range(1, horizon):\n",
    "        for s_prev in range(n_states):\n",
    "            if s_prev in terminal:\n",
    "                continue\n",
    "            for a in range(n_actions):\n",
    "                prob = d[s_prev, t - 1] * policy[s_prev, a]  # probability of policy performing this action\n",
    "                if prob == 0:\n",
    "                    continue\n",
    "                # student code here\n",
    "                \n",
    "                # NOTE: non-vectorized version\n",
    "                # for s in range(n_states):\n",
    "                #     d[s, t] += prob * p_transition[s_prev, s, a]\n",
    "                \n",
    "                d[:, t] += prob * p_transition[s_prev, :, a]  # push (future state) visit probability forward\n",
    "                # end student code\n",
    "    return d.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4e78e2",
   "metadata": {},
   "source": [
    "#### Exponentiated-gradient MaxEnt IRL update\n",
    "\n",
    "At each iteration we compute the gradient\n",
    "$$\\nabla_\\omega = \\mu_E - \\mu_{\\pi_\\omega}$$\n",
    "and apply an exponentiated-gradient step\n",
    "$$\\omega \\leftarrow \\omega \\odot \\exp(\\alpha_t \\nabla_\\omega), \\qquad \\alpha_t = \\frac{\\alpha}{1 + t}.$$\n",
    "This keeps feature weights positive while shrinking the step size over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a3a06d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxent_irl(p_transition: np.ndarray, features: np.ndarray, terminal: Iterable[int], demos: Iterable[Trajectory], lr: float = 0.2, eps: float = 1e-4, max_iter: int = 1000) -> np.ndarray:\n",
    "    demos = list(demos)\n",
    "    empirical = feature_expectation_from_trajectories(features, demos)\n",
    "    p_initial = initial_probabilities_from_trajectories(p_transition.shape[0], demos)\n",
    "\n",
    "    omega = np.ones(features.shape[1], dtype=np.float64)\n",
    "    step = 0\n",
    "    delta = np.inf\n",
    "    while delta > eps and step < max_iter:\n",
    "        omega_old = omega.copy()\n",
    "        reward = features @ omega\n",
    "        expected_svf = compute_expected_svf(p_transition, p_initial, terminal, reward)\n",
    "        model_features = features.T @ expected_svf\n",
    "        # student code here\n",
    "        # TODO: form the feature-expectation gradient and apply an exponentiated-gradient ascent step with a decaying learning rate\n",
    "        raise NotImplementedError(\"TODO: fill in this block\")\n",
    "        # end student code\n",
    "        delta = np.max(np.abs(omega - omega_old))\n",
    "        step += 1\n",
    "    learned_reward = features @ omega\n",
    "    return learned_reward\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b5fce54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Feature expectations: max |Δ|=0.00e+00\n",
      "[OK] Initial-state histogram: sums to 1.0\n",
      "[OK] Initial-state histogram entries: max |Δ|=0.00e+00\n",
      "[OK] Expected SVF sanity check: max |Δ|=0.00e+00\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "TODO: fill in this block",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m svf \u001b[38;5;241m=\u001b[39m compute_expected_svf(p_chain, np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m]), [\u001b[38;5;241m2\u001b[39m], np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m     36\u001b[0m assert_close(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected SVF sanity check\u001b[39m\u001b[38;5;124m\"\u001b[39m, svf, np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m]))\n\u001b[0;32m---> 38\u001b[0m learned_reward \u001b[38;5;241m=\u001b[39m \u001b[43mmaxent_irl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_transition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterminal_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpert_trajectories\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m p_initial_world \u001b[38;5;241m=\u001b[39m initial_probabilities_from_trajectories(world\u001b[38;5;241m.\u001b[39mn_states, expert_trajectories)\n\u001b[1;32m     40\u001b[0m svf_world \u001b[38;5;241m=\u001b[39m compute_expected_svf(world\u001b[38;5;241m.\u001b[39mp_transition, p_initial_world, terminal_states, learned_reward)\n",
      "Cell \u001b[0;32mIn[29], line 16\u001b[0m, in \u001b[0;36mmaxent_irl\u001b[0;34m(p_transition, features, terminal, demos, lr, eps, max_iter)\u001b[0m\n\u001b[1;32m     13\u001b[0m model_features \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m expected_svf\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# student code here\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# TODO: form the feature-expectation gradient and apply an exponentiated-gradient ascent step with a decaying learning rate\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTODO: fill in this block\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# end student code\u001b[39;00m\n\u001b[1;32m     18\u001b[0m delta \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(np\u001b[38;5;241m.\u001b[39mabs(omega \u001b[38;5;241m-\u001b[39m omega_old))\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: TODO: fill in this block"
     ]
    }
   ],
   "source": [
    "# Tests\n",
    "\n",
    "def assert_close(name: str, actual, expected, atol=1e-6):\n",
    "    actual = np.asarray(actual, dtype=float)\n",
    "    expected = np.asarray(expected, dtype=float)\n",
    "    diff = float(np.max(np.abs(actual - expected)))\n",
    "    if diff > atol:\n",
    "        raise AssertionError(f\"{name} mismatch (max |Δ|={diff:.3e}). Expected {expected}, got {actual}\")\n",
    "    print(f\"[OK] {name}: max |Δ|={diff:.2e}\")\n",
    "\n",
    "\n",
    "def assert_probability_vector(name: str, vec):\n",
    "    total = float(np.sum(vec))\n",
    "    if not np.isclose(total, 1.0, atol=1e-6):\n",
    "        raise AssertionError(f\"{name} should sum to 1.0 but sums to {total:.6f}\")\n",
    "    if np.any(vec < -1e-8):\n",
    "        raise AssertionError(f\"{name} contains negative mass: {vec}\")\n",
    "    print(f\"[OK] {name}: sums to 1.0\")\n",
    "\n",
    "\n",
    "simple_features = np.eye(3)\n",
    "traj_a = Trajectory([Transition(0, 0, 1), Transition(1, 0, 2)])\n",
    "traj_b = Trajectory([Transition(0, 0, 0), Transition(0, 0, 1)])\n",
    "fe = feature_expectation_from_trajectories(simple_features, [traj_a, traj_b])\n",
    "assert_close(\"Feature expectations\", fe, np.array([1.5, 1.0, 0.5]))\n",
    "\n",
    "pi0 = initial_probabilities_from_trajectories(3, [traj_a, traj_b])\n",
    "assert_probability_vector(\"Initial-state histogram\", pi0)\n",
    "assert_close(\"Initial-state histogram entries\", pi0, np.array([1.0, 0.0, 0.0]))\n",
    "\n",
    "p_chain = np.zeros((3, 3, 1))\n",
    "p_chain[0, 1, 0] = 1.0\n",
    "p_chain[1, 2, 0] = 1.0\n",
    "p_chain[2, 2, 0] = 1.0\n",
    "svf = compute_expected_svf(p_chain, np.array([1.0, 0.0, 0.0]), [2], np.zeros(3))\n",
    "assert_close(\"Expected SVF sanity check\", svf, np.array([1.0, 1.0, 1.0]))\n",
    "\n",
    "learned_reward = maxent_irl(world.p_transition, features, terminal_states, expert_trajectories)\n",
    "p_initial_world = initial_probabilities_from_trajectories(world.n_states, expert_trajectories)\n",
    "svf_world = compute_expected_svf(world.p_transition, p_initial_world, terminal_states, learned_reward)\n",
    "empirical_features = feature_expectation_from_trajectories(features, expert_trajectories)\n",
    "model_features = features.T @ svf_world\n",
    "max_feature_diff = float(np.max(np.abs(model_features - empirical_features)))\n",
    "print(f\"Max feature expectation gap: {max_feature_diff:.4f}\")\n",
    "if max_feature_diff > 0.85:\n",
    "    raise AssertionError(\n",
    "        \"Learned reward does not yet match demonstration feature expectations closely enough.\"\n",
    "    )\n",
    "print(\"Section 1 tests passed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbe5227",
   "metadata": {},
   "source": [
    "#### Visualizing learned rewards and policies\n",
    "After training, we can visualize the learned reward function and the induced policy. These plots illustrate how well the IRL algorithm has captured the expert's behavior through the inferred reward structure. Your estimated rewards should have the correct top two high reward states, but expect some noise in the lower-value states due to limited demonstration data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ac783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_reward_comparison(world, expert_reward, learned_reward):\n",
    "    side = world.size\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    vmin = min(expert_reward.min(), learned_reward.min())\n",
    "    vmax = max(expert_reward.max(), learned_reward.max())\n",
    "    im0 = axes[0].imshow(expert_reward.reshape(side, side), cmap=\"viridis\", vmin=vmin, vmax=vmax)\n",
    "    axes[0].set_title(\"Expert Reward\")\n",
    "    axes[1].imshow(learned_reward.reshape(side, side), cmap=\"viridis\", vmin=vmin, vmax=vmax)\n",
    "    axes[1].set_title(\"Learned Reward\")\n",
    "    for ax in axes:\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    fig.colorbar(im0, ax=axes.ravel().tolist(), shrink=0.75)\n",
    "    plt.show()\n",
    "\n",
    "plot_reward_comparison(world, expert_reward, learned_reward)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a12e551",
   "metadata": {},
   "source": [
    "## 2. GRPO Fine-Tuning from Scratch [40 pts]\n",
    "\n",
    "1. Conceptual questions [10 pts]\n",
    "2. GRPO implementation [30 pts]\n",
    "\n",
    "Group Relative Policy Optimization (GRPO) extends PPO for RLHF by sampling multiple responses per prompt and using their mean reward as a control variate. This stabilizes policy updates when rewards come from evaluators or heuristics--exactly the setting for aligning language and multimodal models.\n",
    "\n",
    "**Suggested resources.**\n",
    "- Zhihong Shao et al., *DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models*, 2024 (introduces GRPO)\n",
    "- Hugging Face TRL blog, *GRPO: Group Relative Policy Optimization*\n",
    "- lsdefine/simple_GRPO GitHub repository (minimal open-source implementation)\n",
    "\n",
    "The remaining cells guide you through implementing the reward, advantage, and loss components so the GRPO training loop actually runs end-to-end.\n",
    "\n",
    "**GRPO recap (Shao et al., 2024).** For each prompt $i$ we draw a group of $m$ completions $\\{y_{i,j}\\}_{j=1}^m$ from the policy. Rewards $r_{i,j}$ are centered with the group mean $\\bar r_i = \\tfrac{1}{m}\\sum_j r_{i,j}$ to form advantages $A_{i,j} = r_{i,j} - \\bar r_i$. The policy update maximizes the PPO-style surrogate $\\min\\big(r_{i,j}A_{i,j}, \\operatorname{clip}(r_{i,j}, 1 \\pm \\epsilon)A_{i,j}\\big)$ while subtracting a KL penalty $\\beta\\,\\mathrm{KL}(\\pi_\\theta \\Vert \\pi_{\\text{ref}})$ to keep the fine-tuned model close to the frozen reference. This notebook isolates each mathematical piece so you can test it in isolation before chaining them together.\n",
    "\n",
    "**What you are implementing.** You will (a) code the structural reward checker, (b) normalize group advantages, (c) derive the GRPO loss from the log-prob ratios, and (d) plug everything into a gradient-accumulated training loop that mirrors Algorithm 1 in the paper. The surrounding evaluation cells verify that format adherence improves after RL fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bb158d",
   "metadata": {},
   "source": [
    "### 2.a Conceptual Questions [10 pts total; 2 pts each]\n",
    "\n",
    "1. Why does GRPO draw $G>1$ completions per prompt, and what is the prompt-level control variate used to center rewards?\n",
    "<font color=\"blue\">Your answer here.</font>\n",
    "\n",
    "2. A prompt yields rewards $(2, 1, -1)$ with group size $G=3$. Compute the centered advantages for each completion.\n",
    "<font color=\"blue\">Your answer here.</font>\n",
    "\n",
    "3. Midway through training you double $\\beta$ in the GRPO loss. Describe the effect this has on the update dynamics.\n",
    "<font color=\"blue\">Your answer here.</font>\n",
    "\n",
    "4. Starting from $r_t = \\exp(\\log p_\\theta - \\log p_{\\text{ref}})$, write the clipped surrogate used by GRPO and explain how the clipping interacts with the sign of the advantage.\n",
    "<font color=\"blue\">Your answer here.</font>\n",
    "\n",
    "5. Format rewards are dense but answer-correctness rewards are sparse. Propose a shaping scheme that keeps correctness learning signal meaningful and justify it.\n",
    "<font color=\"blue\">Your answer here.</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa14098",
   "metadata": {},
   "source": [
    "### 2.b GRPO Implementation [30 pts]\n",
    "\n",
    "Complete the reward shaping helpers, grouped-advantage computation, GRPO loss, and the short training/eval loop. Tests in Section 2 cover each of these components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9074abbc",
   "metadata": {},
   "source": [
    "### Section 2 implementation guide\n",
    "\n",
    "We now pivot to GRPO fine-tuning. The next block of cells loads libraries, defines the system & user prompts, constructs the reward function, and wires together helper utilities (log-prob extraction, grouped advantages, optimizer setup). Treat those as scaffolding so you can focus on the GRPO-specific TODOs and diagnostics. In particular, the device/import setup, dataset builder, model + LoRA configuration, and format/decoding helper cells are boilerplate—you do not need to edit them.\n",
    "\n",
    "Expect to run the provided GRPO loop on GPU; with the given reward shaping it can drive format adherence close to 100% after enough steps. Format adherence thresholds are tiered: 50% post-RL earns 2 pts, 70% earns 4 pts, and 90% earns the full 6 pts, so feel free to keep training until you hit the ceiling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fff39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e6cdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"You are a helpful assistant. First reason internally, then provide the user answer. \"\n",
    "    \"Respond using the template:\\n\"\n",
    "    \"<think>\\n\"\n",
    "    \"...\\n\"\n",
    "    \"</think>\\n\"\n",
    "    \"<answer>\\n\"\n",
    "    \"...\\n\"\n",
    "    \"</answer>\"\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class QAExample:\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "\n",
    "def build_dataset(num_examples: int = 64) -> Dataset:\n",
    "    records = []\n",
    "    for _ in range(num_examples):\n",
    "        a = random.randint(10, 99)\n",
    "        b = random.randint(1, 9)\n",
    "        question = f\"If you add {a} and {b}, what is the result?\"\n",
    "        answer = str(a + b)\n",
    "        records.append({\"question\": question, \"answer\": answer})\n",
    "    return Dataset.from_list(records)\n",
    "\n",
    "train_ds = build_dataset()\n",
    "train_ds[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc40dc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\")\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model = model.to(device)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c444fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "FORMAT_PATTERN = re.compile(r\"^<think>\\n[\\s\\S]*\\n</think>\\n<answer>\\n[\\s\\S]*\\n</answer>$\", re.MULTILINE)\n",
    "\n",
    "def format_reward(responses: List[str]) -> torch.Tensor:\n",
    "    scores = [1.0 if FORMAT_PATTERN.match(resp.strip()) else 0.0 for resp in responses]\n",
    "    return torch.tensor(scores, dtype=torch.float32, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "format-reward-sanity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks for format_reward\n",
    "valid_resp = \"<think>\\nreasoning goes here\\n</think>\\n<answer>\\nthe meaning of life is\\n</answer>\"\n",
    "invalid_resp = \"<think>missing closing tags\"\n",
    "scores = format_reward([valid_resp, invalid_resp])\n",
    "print('format_reward scores:', scores.cpu().tolist())\n",
    "assert scores[0].item() == 1.0\n",
    "assert scores[1].item() == 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39001330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question: str) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "prompts = [build_prompt(rec[\"question\"]) for rec in train_ds]\n",
    "prompts[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6c6a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def completion_log_probs(model, sequences: torch.Tensor, prompt_lengths: torch.Tensor) -> torch.Tensor:\n",
    "    attention_mask = (sequences != tokenizer.pad_token_id).long()\n",
    "    outputs = model(input_ids=sequences, attention_mask=attention_mask)\n",
    "    logits = outputs.logits[:, :-1, :]\n",
    "    target_ids = sequences[:, 1:]\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "    token_logps = torch.gather(log_probs, dim=-1, index=target_ids.unsqueeze(-1)).squeeze(-1)\n",
    "    cont_logps = []\n",
    "    for i in range(sequences.size(0)):\n",
    "        prompt_len = prompt_lengths[i].item()\n",
    "        total_len = attention_mask[i].sum().item()\n",
    "        start = max(prompt_len - 1, 0)\n",
    "        end = max(total_len - 1, start)\n",
    "        cont_logps.append(token_logps[i, start:end].sum())\n",
    "    return torch.stack(cont_logps)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_group(\n",
    "    model,\n",
    "    prompts: List[str],\n",
    "    num_generations: int = 2,\n",
    "    max_new_tokens: int = 128,\n",
    "    do_sample: bool = True,\n",
    "    temperature: float = 0.9,\n",
    "):\n",
    "    encoded = tokenizer(prompts, return_tensors=\"pt\", padding=True, add_special_tokens=False)\n",
    "    prompt_lengths = encoded[\"attention_mask\"].sum(dim=1)\n",
    "    encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "\n",
    "    gen_kwargs = {\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"do_sample\": do_sample,\n",
    "        \"num_return_sequences\": num_generations,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    }\n",
    "    if do_sample:\n",
    "        gen_kwargs[\"temperature\"] = temperature\n",
    "\n",
    "    outputs = model.generate(**encoded, **gen_kwargs)\n",
    "\n",
    "    batch_size = len(prompts)\n",
    "    sequences = outputs.view(batch_size, num_generations, -1)\n",
    "    seq_list = []\n",
    "    prompt_len_list = []\n",
    "    texts = []\n",
    "    for i in range(batch_size):\n",
    "        prompt_len = prompt_lengths[i].item()\n",
    "        for g in range(num_generations):\n",
    "            ids = sequences[i, g]\n",
    "            mask = (ids != tokenizer.pad_token_id).long()\n",
    "            seq_len = mask.sum().item()\n",
    "            seq_list.append(ids)\n",
    "            prompt_len_list.append(prompt_len)\n",
    "            text_tokens = ids[prompt_len:seq_len]\n",
    "            texts.append(tokenizer.decode(text_tokens, skip_special_tokens=True))\n",
    "    stacked = torch.stack(seq_list).to(device)\n",
    "    prompt_len_tensor = torch.tensor(prompt_len_list, device=device)\n",
    "    rewards = format_reward(texts)\n",
    "    return stacked, prompt_len_tensor, rewards, texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f42a9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_group_advantages(rewards: torch.Tensor, group_size: int) -> torch.Tensor:\n",
    "    if group_size <= 0:\n",
    "        raise ValueError(f\"group_size must be positive; got {group_size}\")\n",
    "    num_rewards = rewards.numel()\n",
    "    if num_rewards % group_size != 0:\n",
    "        raise ValueError(\n",
    "            f\"Reward tensor of length {num_rewards} is not divisible by group_size={group_size}\"\n",
    "        )\n",
    "    reshaped = rewards.view(-1, group_size)\n",
    "    mean = reshaped.mean(dim=1, keepdim=True)\n",
    "    std = reshaped.std(dim=1, keepdim=True, unbiased=False).clamp_min(1e-6)\n",
    "    normalized = (reshaped - mean) / std\n",
    "    return normalized.view(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1197018",
   "metadata": {},
   "source": [
    "#### GRPO clipped surrogate with KL anchor\n",
    "\n",
    "For each sampled sequence we form\n",
    "$$r_t = \\exp(\\log p_\\theta - \\log p_{\\text{ref}}), \\qquad L_{\\text{clip}} = \\min(r_t A_t, \\mathrm{clip}(r_t, 1\\pm\\epsilon) A_t).$$\n",
    "The full loss subtracts a KL penalty against the frozen reference policy:\n",
    "$$\\mathcal{L}_{\\text{GRPO}} = L_{\\text{clip}} - \\beta\\,\\mathrm{KL}(\\pi_\\theta\\,\\|\\,\\pi_{\\text{ref}}).$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c428c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grpo_sequence_loss(logp_new: torch.Tensor, logp_ref: torch.Tensor, advantages: torch.Tensor, beta: float, epsilon: float) -> torch.Tensor:\n",
    "    # student code here\n",
    "    # TODO: compute the clipped PPO-style surrogate and subtract the GRPO KL penalty term\n",
    "    raise NotImplementedError(\"TODO: fill in this block\")\n",
    "    # end student code\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba67280",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "beta = 0.05\n",
    "epsilon = 0.2\n",
    "group_size = 2\n",
    "\n",
    "\n",
    "def grpo_update(batch_prompts: List[str], grad_accum: int | None = None):\n",
    "    accum_steps = grad_accum if grad_accum is not None else 1\n",
    "    if accum_steps < 1:\n",
    "        raise ValueError(\"grad_accum must be >= 1\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    all_rewards: list[torch.Tensor] = []\n",
    "    all_losses: list[torch.Tensor] = []\n",
    "    collected_texts: list[str] = []\n",
    "\n",
    "    for _ in range(accum_steps):\n",
    "        sequences, prompt_lens, rewards, texts = sample_group(\n",
    "            model,\n",
    "            batch_prompts,\n",
    "            num_generations=group_size,\n",
    "        )\n",
    "\n",
    "        logp_new = completion_log_probs(model, sequences, prompt_lens)\n",
    "\n",
    "        adapters_supported = hasattr(model, \"disable_adapter\") and hasattr(model, \"enable_adapter\")\n",
    "        if adapters_supported:\n",
    "            model.disable_adapter()\n",
    "        with torch.no_grad():\n",
    "            logp_ref = completion_log_probs(model, sequences, prompt_lens)\n",
    "        if adapters_supported:\n",
    "            model.enable_adapter()\n",
    "\n",
    "        advantages = compute_group_advantages(rewards, group_size)\n",
    "        base_loss = grpo_sequence_loss(logp_new, logp_ref, advantages, beta=beta, epsilon=epsilon)\n",
    "        (base_loss / accum_steps).backward()\n",
    "\n",
    "        all_losses.append(base_loss.detach())\n",
    "        all_rewards.append(rewards.detach())\n",
    "        collected_texts.extend(texts)\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    reward_tensor = torch.cat(all_rewards)\n",
    "    reward_mean = reward_tensor.mean().item()\n",
    "    reward_std = reward_tensor.std().item() if reward_tensor.numel() > 1 else 0.0\n",
    "    loss_value = torch.stack(all_losses).mean().item()\n",
    "\n",
    "    return {\n",
    "        \"loss\": loss_value,\n",
    "        \"reward_mean\": reward_mean,\n",
    "        \"reward_std\": reward_std,\n",
    "        \"format_match_pct\": reward_mean * 100.0,\n",
    "        \"texts\": collected_texts,\n",
    "        \"optimizer_step\": True,\n",
    "        \"accumulated_microbatches\": accum_steps,\n",
    "        \"grad_accum_steps\": accum_steps,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1572e1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit tests for helpers\n",
    "\n",
    "def check_tensor_close(name: str, actual: torch.Tensor, expected: torch.Tensor, atol: float = 1e-6):\n",
    "    diff = torch.max(torch.abs(actual - expected)).item()\n",
    "    if diff > atol:\n",
    "        raise AssertionError(f\"{name} mismatch (max |Δ|={diff:.3e}). Expected {expected}, got {actual}\")\n",
    "    print(f\"[OK] {name}: max diff={diff:.2e}\")\n",
    "\n",
    "raw_rewards = torch.tensor([1.0, 0.0, 2.0, 4.0], device=device)\n",
    "adv = compute_group_advantages(raw_rewards, group_size=2)\n",
    "reshaped_adv = adv.view(2, 2)\n",
    "check_tensor_close(\"Group advantage mean\", reshaped_adv.mean(dim=1), torch.zeros(2, device=device))\n",
    "check_tensor_close(\"Group advantage std\", reshaped_adv.std(dim=1, unbiased=False), torch.ones(2, device=device))\n",
    "\n",
    "logp_new = torch.log(torch.tensor([0.6, 0.4, 0.7, 0.3], device=device))\n",
    "logp_ref = torch.log(torch.tensor([0.5, 0.5, 0.6, 0.4], device=device))\n",
    "manual_adv = torch.tensor([1.2, -0.4, 0.5, -1.0], device=device)\n",
    "loss = grpo_sequence_loss(logp_new, logp_ref, manual_adv, beta=0.1, epsilon=0.2)\n",
    "check_tensor_close(\"GRPO loss test\", loss, -0.2233469)\n",
    "\n",
    "logp_new_grad = torch.log(torch.tensor([0.55, 0.45, 0.65, 0.35], device=device))\n",
    "logp_new_grad.requires_grad_(True)\n",
    "loss_grad = grpo_sequence_loss(logp_new_grad, logp_ref, manual_adv, beta=0.05, epsilon=0.2)\n",
    "loss_grad.backward()\n",
    "if not torch.all(torch.isfinite(logp_new_grad.grad)):\n",
    "    raise AssertionError(\"Non-finite gradients detected in GRPO loss\")\n",
    "print(f\"[OK] GRPO loss backward pass: grad norm={logp_new_grad.grad.norm().item():.4f}\")\n",
    "\n",
    "print(\"Section 2 helper tests passed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "format-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_format_rate(\n",
    "    model,\n",
    "    dataset,\n",
    "    num_examples: int | None = 32,\n",
    "    num_generations: int = 1,\n",
    "    max_new_tokens: int = 128,\n",
    "    do_sample: bool = False,\n",
    "):\n",
    "    if num_examples is None:\n",
    "        subset = dataset\n",
    "    else:\n",
    "        size = min(num_examples, len(dataset))\n",
    "        subset = dataset.select(range(size))\n",
    "    effective_generations = num_generations\n",
    "    if not do_sample and num_generations != 1:\n",
    "        effective_generations = 1\n",
    "    prompts = [build_prompt(ex[\"question\"]) for ex in subset]\n",
    "    _, _, rewards, texts = sample_group(\n",
    "        model,\n",
    "        prompts,\n",
    "        num_generations=effective_generations,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "    )\n",
    "    total = rewards.numel()\n",
    "    matches = rewards.sum().item()\n",
    "    rate = (matches / total) * 100 if total else 0.0\n",
    "    print(f\"Format adherence: {rate:.1f}%\")\n",
    "    return rate, texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a05ffad",
   "metadata": {},
   "source": [
    "### Format evaluation and demo training loop\n",
    "\n",
    "After defining the helpers, we first measure baseline format adherence, then run a short GRPO training loop. Aim for at least 90% post-training adherence to receive full credit (the performance can climb near 100% after 25 steps if done correctly). Training should take approximately 30 minutes in Colab with a T4 GPU.\n",
    "\n",
    "Again, 50% post-RL earns 2 pts, 70% earns 4 pts, and 90% earns the full 6 pts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6a6f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = deque(maxlen=10)\n",
    "eval_subset = train_ds.select(range(min(32, len(train_ds))))\n",
    "print(\"Baseline format adherence (pre-RL):\")\n",
    "baseline_rate, _ = evaluate_format_rate(\n",
    "    model,\n",
    "    eval_subset,\n",
    "    num_examples=None,\n",
    "    num_generations=group_size,\n",
    "    do_sample=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a036a683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "for step in range(25):\n",
    "    batch = train_ds.shuffle(seed=step).select(range(4))  # batch size 4\n",
    "    prompts_batch = [build_prompt(ex[\"question\"]) for ex in batch]\n",
    "    stats = grpo_update(prompts_batch, grad_accum=8)  # simulate batch size 4x8=32\n",
    "    history.append(stats[\"reward_mean\"])\n",
    "    print(\n",
    "        f\"Step {step + 1:02d} | reward_mean={stats['reward_mean']:.3f} | format adherence={stats['format_match_pct']:.1f}% | time={(time.time() - start_time):.1f}s\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3827bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Format adherence after RL:\")\n",
    "post_rate, _ = evaluate_format_rate(\n",
    "    model,\n",
    "    eval_subset,\n",
    "    num_examples=None,\n",
    "    num_generations=group_size,\n",
    "    do_sample=False,\n",
    ")\n",
    "print(f\"Baseline: {baseline_rate:.1f}% | Post-RL: {post_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae9805d",
   "metadata": {},
   "source": [
    "## Submission Checklist \n",
    "\n",
    "- All answers filled.\n",
    "- Section 1 tests succeed; visualization optional.\n",
    "- Section 2 helper tests succeed; optional demo loop left commented.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bc76d1",
   "metadata": {},
   "source": [
    "### Chain-of-Thought Playground Example\n",
    "\n",
    "Use this cell to poke the fine-tuned model with your own prompts. We start with a simple 4-digit addition problem so you can see the expected <think>/<answer> chain of thought. Edit `example_question` and re-run as you like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4342a3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_question = \"What's 4821 + 7394?\"\n",
    "example_prompt = build_prompt(example_question)\n",
    "print(\"Prompt:\", example_question)\n",
    "\n",
    "inputs = tokenizer(example_prompt, return_tensors=\"pt\", padding=True, add_special_tokens=False).to(device)\n",
    "input_len = inputs[\"input_ids\"].shape[1]\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "new_tokens = output_ids[0, input_len:]\n",
    "completion = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "print(\"Model completion:\")\n",
    "print(completion.strip())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ofEJa44_c8i"
      },
      "source": [
        "# HW3 - Model Based RL - Neural Dynamics Modeling, CEM, PETS\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2IwS_KnpqJL"
      },
      "source": [
        "#### Background"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPVkUrW6pUt_"
      },
      "source": [
        "**Model free vs model based**\n",
        "\n",
        "HW1 and HW2 explored **model free RL**. In that setting, we don't learn a dynamics model or plan using a dynamics model - hence model free. Theoretically, model free MDP exploration takes place directly in the real world - in practice this only works for **perfect simulators** where the training environment exactly matches the testing environment, like video games or board games. These approaches **learn policies** that map all states to optimal actions.\n",
        "\n",
        "This assignment explores **model based RL**. In this setting we **learn models** of the approximate transition dynamics from data and **plan actions with those models** - hence model based. This is required for robotics, autonomous driving, or any hardware system that is impossible to simulate perfectly.\n",
        "\n",
        "Note, there is also a broad gray area between model based and model free rl. You can approximate dynamics and then learn a policy, you can plan trajectories in a perfect simulator, etc. This distinction between the algorithm classes is more historical than deeply significant.\n",
        "\n",
        "**HW3**\n",
        "\n",
        "This assignment builds to an important recent work in model based RL - [PETS](https://arxiv.org/abs/1805.12114): probabilistic ensembles with trajectory sampling (2018). It progresses from predecessor techniques: deterministic neural dynamics modeling (\\~1990), Cross Entropy Method (\\~1999), and stochastic neural dynamics modeling (\\~1994).\n",
        "\n",
        "For this assignment, we pretend our [inverted pendulum](https://gymnasium.farama.org/environments/mujoco/inverted_pendulum/) environment is an expensive hardware system we have previously collected data from. From that data we will train models, and with those models we will plan actions.\n",
        "\n",
        "Note, variations of these algorithms exist. Please use the math contained in this notebook for the coding sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PHhuXXvnxb8"
      },
      "source": [
        "# 0. Warm Up Questions [30 pts total; 2 pt each]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaT41gO4n0zq"
      },
      "source": [
        "1.    What're the differences between an optimal policy and an optimal trajectory?<br>\n",
        "\n",
        "> An optimal policy $\\pi$ knows how to create an optimal trajectory $\\tau^*$, given different self/environment states. \n",
        "\n",
        "\n",
        "2.    In model free RL we search over all possible policy parameters $\\theta$ for optimal parameters $\\theta^*$ which result in the highest sum of rewards:\n",
        "$$\n",
        "\\theta^* = \\arg \\max_\\theta \\displaystyle\\sum_{t=1}^{T} r(s_t, a_t)\n",
        "$$\n",
        "In basic language, what is the meaning of this constrained optimization below, which is used to plan trajectories in model based RL? [hint: the deterministic case](https://www.youtube.com/watch?v=4SL0DnxC1GM&list=PL_iWQOsE6TfVYGEGiAOMaOzzv41Jfm_Ps&index=40)\n",
        "$$\n",
        "a_1^*, ..., a^*_T = \\arg \\max_{a_1, ..., a_T} \\displaystyle\\sum_{t=1}^T r(s_t, a_t)\\\\\n",
        "\\text{  subject to:  } s_{t+1} = s_t + f_\\theta(s_t, a_t)\n",
        "$$ <br>\n",
        "\n",
        "> The optimal trajectory ($a_1^*, ..., a^*_T$) is a sequence of actions that produces the maximum expected rewards ($\\sum r(s_t, a_t)$) given an environment whose state update (transition dynamics) is governed by the function $f_\\theta(s_t, a_t)$.\n",
        "\n",
        "\n",
        "2.    Why is it helpful to replan at every time step in a Model Predictive Control settings? [hint: What if we make a mistake](https://www.youtube.com/watch?v=LkTmiylbHYk&list=PL_iWQOsE6TfVYGEGiAOMaOzzv41Jfm_Ps&index=45)<br>\n",
        "\n",
        "> It allows the system to reduce the effects of error accumulation due to uncertainty in sensing and dynamics prediction — essentially turning it into a closed-loop feedback system (as opposed to open-loop/blind rollout). \n",
        "\n",
        "\n",
        "3.    What're the 3 basic steps of model based RL? [hint: mbrl v 0.5](https://www.youtube.com/watch?v=LkTmiylbHYk&list=PL_iWQOsE6TfVYGEGiAOMaOzzv41Jfm_Ps&index=46)<br>\n",
        "\n",
        "> 1. Collect data using some policy. 2. Obtain and/or learn the dynamics model. 3. Plan and roll out actions using the dynamics model and a planning algorithm.\n",
        "\n",
        "4.    Write a valid equation for mean squared error loss for a neural approximation to deterministic transition dynamics. Define any variables you use.<br>\n",
        "\n",
        "> $\\mathcal{L} = \\frac{1}{N} \\sum (s_{t+1} - \\hat{s_{t+1}})^2$, where $\\hat{s_{t+1}}$ is the predicted environment state at next step, $s_{t+1}$ is the actual environment state, and $N$ is the time horizon.\n",
        "\n",
        "5.    What're the two steps in the \"guess & check\" method of stochastic optimization? Define any variables you use. [hint](https://www.youtube.com/watch?v=pd9mKcH4kkk&list=PL_iWQOsE6TfVYGEGiAOMaOzzv41Jfm_Ps&index=41)<br>\n",
        "\n",
        "> 1. Randomly sample action trajectories: $$ \n",
        "> 2. Calculate reward and pick the best one(s): $$\n",
        "\n",
        "6.    In general machine learning, what is an evolutionary algorithm?<br>\n",
        "\n",
        "> A class of algorithms that iteratively generate a set of candidate solutions via random sampling and use \"natural selection\" as the update rule. \n",
        "\n",
        "7.    How does the cross entropy method improve on \"guess & check\"? What're the four steps of the cross entropy method? [hint](https://www.youtube.com/watch?v=pd9mKcH4kkk&list=PL_iWQOsE6TfVYGEGiAOMaOzzv41Jfm_Ps&index=41)<br>\n",
        "\n",
        "> 1. Sample trajectories (action sequences) from a Gaussian distribution.\n",
        "> 2. Calculate rewards to find the best performing (elite) trajectories.\n",
        "> 3. Fit the Gaussian parameters ($\\mu$, $\\sigma$) to the elite trajectories.\n",
        "> 4. Generate new sample trajectories using the calculated Gaussian parameters. \n",
        "> 5. Repeat!\n",
        "\n",
        "8.    In guess and check or CEM, you have an environment with 5 continuous states and 3 continuous actions and you want to randomly sample 1000 trajectories, each 15 timesteps long. What size must your mean and standard deviation tensors be to allow this sampling?<br>\n",
        "\n",
        "> shape (3, 15), size 45. \n",
        "\n",
        "> Q. Why are different mean/std stored per timestep? Why not just a single scalar number for each?\n",
        "\n",
        "9.    Alea is latin for dice. What is aleatoric uncertainty? <br>\n",
        "\n",
        "> Uncertainty due to the **inherent complexity** of the environmental state. Cannot be reduced just from more data collection. \n",
        "\n",
        "10.    What type of loss should you use to fit the parameters of a guassian to a set of random samples from a guassian? and why not just use MSE? [hint](https://pytorch.org/docs/stable/generated/torch.nn.GaussianNLLLoss.html)<br>\n",
        "\n",
        "> Gaussian Negative Log-Likelihood. Basically, retains standard deviation (whereas `MSE` assumes homoscedastic, or uniform variance) information to reason about the uncertainty of the prediction. \n",
        "\n",
        "11.    Episteme is greek for knowledge. What is epistemic uncertainty? <br>\n",
        "\n",
        "> Uncertainty due to model or data imperfections. Thus, can be reduced with better data or model. \n",
        "\n",
        "12.    In general machine learning, what is an ensemble?<br>\n",
        "\n",
        "> A class of techniques that involve independent generation of a set of (intermediary) output value candidates and holistically combine them to obtain a better accuracy (vs. without ensembling). \n",
        "\n",
        "13.    Why is sequential dynamics modeling harder than non sequential supervised learning?<br>\n",
        "\n",
        "> Because there is an added variable of time and state history/memory. For one, there is the problem of **error accumulation**. For another, the inputs are \"correlated\" (depend on previous states). \n",
        "\n",
        "14. Explain the problem of distributional shift in model based RL, and write the equation that summarizes this phenomena. [hint: Does it work? No!](https://www.youtube.com/watch?v=LkTmiylbHYk&list=PL_iWQOsE6TfVYGEGiAOMaOzzv41Jfm_Ps&index=45)<br>\n",
        "\n",
        "> The distribution of data it saw during training does not match that of inference. In other words, the model now needs to extrapolate in data domains it has not seen before, instead of interpolating between known examples. $p_{model}(s_t) \\neq p_{env}(s_t)$.\n",
        "\n",
        "15.    Why is uncertainty estimation important in model based RL? [hint: how uncertainty estimation can help](https://www.youtube.com/watch?v=pSvjDO1B9WY&list=PL_iWQOsE6TfVYGEGiAOMaOzzv41Jfm_Ps&index=47)<br>\n",
        "\n",
        "> Because the uncertainty provides valuable information in practice — like how safe it is to deploy, where the data is most lacking, etc.\n",
        "> Also, it can prevent the model from chasing \"spurious peaks\" — essentially fake positive states with overestimated rewards (but low confidence).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPnPio6d94fy"
      },
      "source": [
        "# Boiler plate\n",
        "\n",
        "Read through at least once.\n",
        "\n",
        "**Will hang until you upload the replay buffer** (every time you restart your runtime)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "NYFcc7tM6LGF"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# @title Imports\n",
        "\n",
        "!pip install gymnasium[mujoco]\n",
        "!apt install -y libgl1-mesa-glx libosmesa6 libglfw3 patchelf\n",
        "import gymnasium as gym\n",
        "\n",
        "import torch\n",
        "from torch import nn, zeros\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from collections import deque\n",
        "import random\n",
        "import copy\n",
        "\n",
        "# code should work on either, faster on gpu\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# random seeds for reproducability\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "random.seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "pkZOQqyvnoNB"
      },
      "outputs": [],
      "source": [
        "# @title Define Replay Buffer\n",
        "class ReplayBuffer:\n",
        "    def __init__(self):\n",
        "        self.buffer = deque(maxlen=6000)\n",
        "        self.batch_size = 32\n",
        "\n",
        "    def store(self, state, action, reward, next_state, done):\n",
        "        transitions = list(zip(state, action, reward, next_state, 1 - torch.Tensor(done)))\n",
        "        self.buffer.extend(transitions)\n",
        "\n",
        "    def sample(self):\n",
        "        batch = random.sample(self.buffer, self.batch_size)\n",
        "        # generic replay buffer from hw2, modified to only return (s, a, s')\n",
        "        states, actions, rewards, next_states, not_dones = [torch.stack(e).to(device) for e in zip(*batch)]\n",
        "        return states, actions, next_states\n",
        "\n",
        "replay_buffer = ReplayBuffer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "i4SoSE7i4vmI"
      },
      "outputs": [],
      "source": [
        "# # @title Upload Replay Buffer\n",
        "# from google.colab import files\n",
        "# import pickle\n",
        "\n",
        "# # Prompt the user to upload the file\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# # Load the replay buffer from the uploaded file\n",
        "# file_name = list(uploaded.keys())[0]  # Get the file name\n",
        "# with open(file_name, 'rb') as f:\n",
        "#     replay_buffer = pickle.load(f)\n",
        "\n",
        "# print(f\"Replay buffer loaded with {len(replay_buffer.buffer)} transitions!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Replay buffer loaded with 6000 transitions!\n"
          ]
        }
      ],
      "source": [
        "# @title Upload Replay Buffer\n",
        "import pickle\n",
        "\n",
        "# Load the replay buffer from the uploaded file\n",
        "file_name = \"./replay_buffer_hw3.pkl\"\n",
        "with open(file_name, 'rb') as f:\n",
        "    replay_buffer: ReplayBuffer = pickle.load(f)\n",
        "\n",
        "print(f\"Replay buffer loaded with {len(replay_buffer.buffer)} transitions!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "U73-XPSasnHQ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
          ]
        }
      ],
      "source": [
        "# @title Visualization Code\n",
        "import os\n",
        "from gym.wrappers import RecordVideo\n",
        "from IPython.display import Video, display, clear_output\n",
        "\n",
        "# Force MuJoCo to use EGL for rendering (important for Colab)\n",
        "# os.environ[\"MUJOCO_GL\"] = \"egl\" # TEMPDEAC\n",
        "\n",
        "def visualize(agent):\n",
        "    \"\"\"Visualize agent with a custom camera angle.\"\"\"\n",
        "\n",
        "    # Create environment in rgb_array mode\n",
        "    env = gym.make(\"InvertedPendulum-v5\", render_mode=\"rgb_array\", reset_noise_scale=0.1, frame_skip=5)\n",
        "\n",
        "    # Apply video recording wrapper\n",
        "    env = RecordVideo(env, video_folder=\"./\", episode_trigger=lambda x: True)\n",
        "\n",
        "    obs, _ = env.reset()\n",
        "\n",
        "    # Access the viewer object through mujoco_py\n",
        "    viewer = env.unwrapped.mujoco_renderer.viewer  # Access viewer\n",
        "    viewer.cam.distance = 3.0     # Set camera distance\n",
        "    viewer.cam.azimuth = 90       # Rotate camera around pendulum\n",
        "    viewer.cam.elevation = 0   # Tilt the camera up/down\n",
        "\n",
        "\n",
        "    for t in range(200):\n",
        "        with torch.no_grad():\n",
        "            actions = agent.get_action(torch.Tensor(obs).to(device)[None, :])[:, 0]\n",
        "        obs, _, done, _= env.step(actions.cpu().numpy())\n",
        "        if done:\n",
        "            break\n",
        "    env.close()\n",
        "\n",
        "    # Display the latest video\n",
        "    clear_output(wait=True)\n",
        "    display(Video(\"./rl-video-episode-0.mp4\", embed=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "Xh2gShdf5Ifl"
      },
      "outputs": [],
      "source": [
        "# @title Evaluation Code\n",
        "\n",
        "def evaluate(agent):\n",
        "\n",
        "    # Create environment in rgb_array mode\n",
        "    env = gym.make(\"InvertedPendulum-v5\", reset_noise_scale=0.1, frame_skip=5)\n",
        "\n",
        "    n = 3\n",
        "    mean_duration = 0\n",
        "    for i in range(n):\n",
        "        obs, _ = env.reset()\n",
        "        done, t = False, 0\n",
        "        while not done and t < 200:\n",
        "            with torch.no_grad():\n",
        "                actions = agent.get_action(torch.Tensor(obs).to(device)[None, :])[:, 0]\n",
        "            obs, _, done, _, _ = env.step(actions.cpu().numpy())\n",
        "            t += 1\n",
        "\n",
        "        mean_duration += t\n",
        "        print(f\"trial {i+1}/{n} lasted {t*.1:.3f} seconds\")\n",
        "\n",
        "    env.close()\n",
        "    print(f\"\\nmean duration: {(mean_duration * .1 / n):.3f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek7oKfhX_Umm"
      },
      "source": [
        "#Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YNkdtlB5acUS"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-629f6fbed82c07cd\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-629f6fbed82c07cd\");\n",
              "          const url = new URL(\"/\", window.location);\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Launch TensorBoard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV-bVS5d-F6C"
      },
      "source": [
        "# Deterministic Dynamics [20 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTGB_0vkIpmx"
      },
      "source": [
        "#### Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMAcv9gF33ZP"
      },
      "source": [
        "\n",
        "1.   Define the dynamics network $f_\\theta$ and optimizer [5 pts]\n",
        "2.   Define `predict()` and `get_loss()` [5 pts]\n",
        "3.   Finish the training code and run training (code cell below unit test) [5 pts]\n",
        "3.   Conceptual question [5 pts]\n",
        "________________________________________\n",
        "Deterministic Neural Dynamics Modeling\n",
        "\n",
        "Let's start by training a neural network to represent the dynamics of the cartpole system from HW2 using 10 mintues of data collected at 10 Hz from a single robot (6000 transitions). We can assume the data came from a human attemping to control the cartpole with a joystick, but it's suboptimal in terms of performance.\n",
        "\n",
        "We will model the transition function as:\n",
        "\n",
        "$$\n",
        "s_{t+1} = s_t + f_\\theta(s_t, a_t)\n",
        "$$\n",
        "Or equivalently:\n",
        "$$\n",
        "s_{t+1} - s_t = f_\\theta(s_t, a_t)\n",
        "$$\n",
        "\n",
        "Train $f_\\theta(s_t, a_t)$ with Mean Squared Error loss on minibatches from the dataset of transition tuples $\\mathcal{D} = \\{(s, a, s')\\}$\n",
        "\n",
        "_______________________________________\n",
        "\n",
        "Deterministic Neural Dynamics Modeling Loss:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\theta) = \\mathbb{E}[\\{(s_{t+1} - s_t)  - f_\\theta(s_t, a_t)\\}^2]\n",
        "$$\n",
        "\n",
        "\n",
        "Where:\n",
        "- $ \\mathcal{L} $ is the dynamics net loss; a function of network parameters $\\theta$\n",
        "-$s_{t+1}$ is state at timestep $t+1$\n",
        "-$s_t$ is action at timestep $t$\n",
        "-$f_\\theta(s_t, a_t)$ is the dynamics network parametrized by $\\theta$\n",
        "-$\\mathbb{E}$ is the expectation or average over the minibatch\n",
        "\n",
        "(hint: No for loops. Use torch's batched operations for greater training speed.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIjmwKUuIxFl"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YBr9aIQXNrgA"
      },
      "outputs": [],
      "source": [
        "n_hidden = 100\n",
        "\n",
        "class DeterministicDynamics():\n",
        "    # NOTE: experiment with learning rate and model size, but keep it same for all models\n",
        "    def __init__(self, n_obs, n_actions):\n",
        "        torch.manual_seed(0)\n",
        "        \n",
        "        self.model = nn.Sequential(\n",
        "          nn.Linear(n_obs + n_actions, n_hidden),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(n_hidden, n_hidden),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(n_hidden, n_obs),\n",
        "          # nn.ReLU(), # turned off, since transition dynamics is delta-encoded\n",
        "        ).to(device)\n",
        "        # end student code\n",
        "\n",
        "    def predict(self, states, actions):\n",
        "        return self.model(torch.cat([states, actions], dim=-1))\n",
        "        #end student code\n",
        "\n",
        "    def get_loss(self, states, actions, next_states):\n",
        "        delta_states = next_states - states\n",
        "        loss = F.mse_loss(self.predict(states, actions), delta_states)\n",
        "        return loss\n",
        "        # end student code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cellView": "form",
        "id": "ObI1SLjMCDCd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test passed: Deterministic dynamics predict appears correct!\n",
            "Test passed: Deterministic dynamics loss appears correct!\n"
          ]
        }
      ],
      "source": [
        "# @title Unit Tests\n",
        "\n",
        "def determ():\n",
        "    torch.manual_seed(0)\n",
        "    # these dont match an actual rollout..\n",
        "    # print debug values during training loop rather than unit tests\n",
        "    batch_size, n_obs, n_actions = 2, 4, 1\n",
        "    s = torch.rand((batch_size, n_obs))\n",
        "    a = torch.rand((batch_size, n_actions))\n",
        "    s_ = torch.rand((batch_size, n_obs))\n",
        "\n",
        "    dyn = DeterministicDynamics(4, 1)\n",
        "    torch.manual_seed(0)\n",
        "    dyn.model = nn.Linear(5, 4) # you should not use this architecture..\n",
        "\n",
        "    delta = dyn.predict(s, a)\n",
        "    # print(delta)\n",
        "    expected_delta = torch.tensor([[ 0.1906,  0.5041, -0.3875,  0.3737],\n",
        "        [-0.2708,  0.6156, -0.7808,  0.1884]])\n",
        "    assert torch.allclose(delta, expected_delta, atol=1e-4), \\\n",
        "    \"Deterministic dynamics predict does not match expected value.\"\n",
        "    print(\"Test passed: Deterministic dynamics predict appears correct!\")\n",
        "\n",
        "\n",
        "    loss = dyn.get_loss(s, a, s_)\n",
        "    # print(loss)\n",
        "    assert abs(loss.item() - (0.3435)) < 1e-4, \\\n",
        "    \"Deterministic dynamics loss does not match expected value.\"\n",
        "    print(\"Test passed: Deterministic dynamics loss appears correct!\")\n",
        "\n",
        "determ()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "sZTnCiJV-mS9"
      },
      "outputs": [],
      "source": [
        "writer = SummaryWriter(log_dir=f'runs/deterministic')\n",
        "# NOTE: if you need extra hints here check out the analagous code in HW1 and HW2\n",
        "\n",
        "# params\n",
        "env = gym.make(\"InvertedPendulum-v5\")\n",
        "lr = 1e-4\n",
        "\n",
        "n_obs = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.shape[0]\n",
        "deterministic_dynamics = DeterministicDynamics(n_obs, n_actions)\n",
        "deterministic_dynamics.model.train()\n",
        "optimizer = torch.optim.AdamW(deterministic_dynamics.model.parameters(), lr=lr, weight_decay=1e-2)\n",
        "\n",
        "for i in range(30_000): # train over 30,000 minibatch samples\n",
        "    states, actions, next_states = replay_buffer.sample()\n",
        "    \n",
        "    states = states.to(device)\n",
        "    actions = actions.to(device)\n",
        "    next_states = next_states.to(device)\n",
        "    \n",
        "    # pred = deterministic_dynamics.predict(states, actions)\n",
        "    optimizer.zero_grad()  # reset gradient\n",
        "    loss = deterministic_dynamics.get_loss(states, actions, next_states)\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(deterministic_dynamics.model.parameters(), 1.0)  # optional\n",
        "    optimizer.step()\n",
        "\n",
        "    # end student code\n",
        "    writer.add_scalar(\"stats/mse_loss\", loss.item(), i)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_3kToTl8AOw"
      },
      "source": [
        "#### Conceptual Question\n",
        "\n",
        "1. In the context of RL for physical control, why learn a dynamics model rather than (1) training directly on a hardware system or (2) deriving a simulator for our system by hand?<br>\n",
        "\n",
        "> (1) Direct training on hardware system is likely to be prohibitively expensive and dangerous (i.e., you *don't* want to roll out random actions into a physical robot). \n",
        "\n",
        "> (2) Not all physical phenomena can be easily implemented in a physics simulator; for example — fluid and soft-body mechanics are extremely hard to derive/integrate, even numerically. Model-based RL allows you to insert some of the knowledge you know (e.g., physical equations derived by great mathematicians and physicists) while retaining the data-driven aspect that is suited for handling real-world chaos and complexity. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQj9reBb_tV7"
      },
      "source": [
        "# Cross Entropy Method [20 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZQDZHPrPs9E"
      },
      "source": [
        "#### Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O29F34w3Ry-0"
      },
      "source": [
        "\n",
        "1.   Finish implementing CEM `get_action()` and run deterministic CEM [15 pts]\n",
        "2.   Conceptual question [5 pts]\n",
        "_______________________________\n",
        "\n",
        "Background\n",
        "\n",
        "Now that we have a model, it's time to plan. We will use the Cross Entropy Method - possibly the simplest online planning algorithm. It's a sampling based evolutionary algorithm that iteratively reduces the cross entropy between our distribution over actions and the optimal disribution over actions at each time step. We start with a random distribution over actions, sample from it, simulate the outcomes and calculate their returns, grab a subset of the best performing actions, and use those to refit our random distribution over actions. Repeat until convergence (in theory) or for a fixed number of iterations (in practice). Replan at every timestep. For further explanation check out the hints in the warm up questions.\n",
        "\n",
        "We are doing open loop planning by solving this constrained optimization:\n",
        "\n",
        "$$\n",
        "a_1^*, ..., a_T^* = \\arg\\max_{a_1, ..., a_T} \\sum_{t=1}^{T} r_t  \\\\\n",
        "\\quad \\quad \\text{subject to} \\quad s_{t+1} = s_t + f_\\theta(s_t, a_t)\n",
        "$$\n",
        "\n",
        "_______________________________\n",
        "Pseudocode for CEM\n",
        "\n",
        "\n",
        "A. Initialize distribution\n",
        "- Create mean = 0, std_dev = 1 tensors for T timesteps and n_action dimensions  (done for you)\n",
        "\n",
        "B. Loop over CEM iterations:  \n",
        "\n",
        "- Randomly sample n_samples parallel action sequences (done for you)\n",
        "\n",
        "- Loop over timesteps T\n",
        "     - Simulate the action sequences in parallel using $f_\\theta$   \n",
        "     - Use `torch.no_grad()` to disable gradient tracking\n",
        "     - Accumulate undiscounted returns at each timestep\n",
        "\n",
        "- Select elite actions:  \n",
        "     - Choose the n_elite action sequences with the highest returns\n",
        "\n",
        "- Refit distribution:  \n",
        "     - Refit mean and std_dev to the n_elite actions\n",
        "     - Clamp the minimum std_dev to .5 to prevent collapse and ensure exploration\n",
        "\n",
        "C. MPC return\n",
        "- Return the mean action for the first timestep only\n",
        "- It should be a tensor of size `torch.Size([1, n_actions])`, since our batching dimension is 1\n",
        "\n",
        "(hint: understand dimensions of all tensors)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "         \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVErJJ_cRzbu"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alm4mgyipEhX"
      },
      "outputs": [],
      "source": [
        "from torch.distributions import Normal\n",
        "\n",
        "class CEM:\n",
        "    def __init__(self, dynamics):\n",
        "        self.dynamics: DeterministicDynamics = dynamics\n",
        "        self.T = 10  # planning time horizon\n",
        "        self.n_actions = 1\n",
        "        self.cem_iter = 20\n",
        "        self.n_samples = 1000\n",
        "        self.n_elite = 200\n",
        "\n",
        "    def compute_rewards(self, state):\n",
        "        reward_t = -state[..., 0].square() / 5.  # incremental position penalty\n",
        "        reward_t -= state[..., 1].square()  # incremental angle penalty\n",
        "        # angle and position limits\n",
        "        soft_done = torch.logical_or(state[..., 1].abs() > 0.2, state[..., 0].abs() > 1.)\n",
        "        reward_t +=  1 - 2 * soft_done.float()\n",
        "        return reward_t\n",
        "\n",
        "    def get_action(self, initial_states):\n",
        "        mean = torch.zeros((self.T, self.n_actions))\n",
        "        std_dev = torch.ones((self.T, self.n_actions))\n",
        "\n",
        "        for i in range(self.cem_iter):\n",
        "            # initial random rollout (before CEM)\n",
        "            states = initial_states.repeat(self.n_samples, 1)\n",
        "            actions = Normal(mean, std_dev).sample((self.n_samples,)).clamp(-3, 3).to(device)\n",
        "            returns = torch.zeros((self.n_samples,))\n",
        "\n",
        "            # roll out actions in batch (for T steps)\n",
        "            for j in range(self.T):  # using for loop since chronological dependency and low loop repeat n\n",
        "                with torch.no_grad():\n",
        "                    delta_states_pred = self.dynamics.predict(states, actions[:,j,...])  # sa: state & action\n",
        "                    next_states_pred = states + delta_states_pred\n",
        "                states = next_states_pred  # alias (for loop update)\n",
        "                returns += self.compute_rewards(states)\n",
        "\n",
        "            # choose elites & re-fit\n",
        "            returns_sorted, idx_sorted = returns.sort(dim=0, descending=True)\n",
        "            elite_actions = actions[idx_sorted[:self.n_elite], ...]\n",
        "            elite_std, elite_mean = torch.std_mean(elite_actions, dim=0, unbiased=False)\n",
        "            mean, std_dev = elite_mean, elite_std  # alias (for loop update)\n",
        "            std_dev = std_dev.clamp_min(0.5)  # min_clamp std to ensure exploration\n",
        "\n",
        "        # max-pool action wrt reward from last distribution\n",
        "        # return actions[idx_sorted[0], 0, ...].unsqueeze(-1)\n",
        "        \n",
        "        # mean-pool action wrt reward from final elite distribution\n",
        "        # NOTE: the closest to the unit test's expected value... altho there's numerical differences for some reason\n",
        "        return mean[0].view(1, self.n_actions)\n",
        "        \n",
        "        # sample from final optimized distribution\n",
        "        # NOTE: why doesn't this work?!\n",
        "        action = Normal(mean, std_dev).sample((1,)).clamp(-3, 3).to(device)\n",
        "        return action[:,0,...]\n",
        "        # end student code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jI2wdtTGAUn3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-1.440662145614624\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "Action value appears wrong",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 29\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(a\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m-\u001b[39m  (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.4642404317855835\u001b[39m)) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-4\u001b[39m, \\\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction value appears wrong\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest passed: CEM Action appears correct!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mcem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[12], line 21\u001b[0m, in \u001b[0;36mcem\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(a\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(a\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m-\u001b[39m  (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.4559649229049683\u001b[39m)) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-4\u001b[39m, \\\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction value appears wrong\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m#todo fix this...\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(a\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m-\u001b[39m  (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.4642404317855835\u001b[39m)) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-4\u001b[39m, \\\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAction value appears wrong\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Action value appears wrong"
          ]
        }
      ],
      "source": [
        "# @title Unit Tests\n",
        "\n",
        "def cem():\n",
        "    torch.manual_seed(0)\n",
        "    # these dont match an actual rollout..\n",
        "    # print debug values during training loop rather than unit tests\n",
        "    batch_size, n_obs = 1, 4\n",
        "    s = 2*torch.rand((batch_size, n_obs)).to(device)-3\n",
        "\n",
        "    dyn = DeterministicDynamics(4, 1)\n",
        "    torch.manual_seed(0)\n",
        "    dyn.model = nn.Linear(5, 4).to(device) # you should not use this architecture..\n",
        "    cem = CEM(dyn)\n",
        "    a = cem.get_action(s)\n",
        "\n",
        "    assert a.shape == torch.Size([1, 1]), \\\n",
        "    \"Action size appears wrong\"\n",
        "\n",
        "    print(a.item())\n",
        "    if device == 'cpu':\n",
        "        assert abs(a.item() -  (-1.4559649229049683)) < 1e-4, \\\n",
        "        \"Action value appears wrong\"\n",
        "    else: #todo fix this...\n",
        "        assert abs(a.item() -  (-1.4642404317855835)) < 1e-4, \\\n",
        "        \"Action value appears wrong\"\n",
        "\n",
        "    print(\"Test passed: CEM Action appears correct!\")\n",
        "\n",
        "cem()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAps_QiN-XCO"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'deterministic_dynamics' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cem \u001b[38;5;241m=\u001b[39m CEM(\u001b[43mdeterministic_dynamics\u001b[49m)\n\u001b[1;32m      2\u001b[0m visualize(cem)\n\u001b[1;32m      3\u001b[0m evaluate(cem)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'deterministic_dynamics' is not defined"
          ]
        }
      ],
      "source": [
        "cem = CEM(deterministic_dynamics)\n",
        "visualize(cem)\n",
        "evaluate(cem)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnva8EL_KViw"
      },
      "source": [
        "#### Conceptual Questions\n",
        "1. What're the pros and cons of real time planning (like this CEM method) vs policy learning (like REINFORCE, DQN, PPO, etc)?<br>\n",
        "\n",
        "> **Pros**: Real-time planning methods only require the dynamics model, instead of a learned policy — in certain cases, like if the reward function is very variable and requires tuning on the spot, CEM would be a lot faster to adapt, since it doesn't require re-training. If dynamics is provided from hand-derived equations, then it's entirely training-free. \n",
        "\n",
        "> **Cons**: It is inefficient at inference time, since it involves massively parallel sampling at every timestep. Due to the exponential branching and compute complexity, it is hard to scale its time horizon to very long durations. Furthermore, if the dynamics is inaccurate, the model will not perform well. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRf3Kl0sERdX"
      },
      "source": [
        "# Stochastic Dynamics [15 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTXy-sDbPnu7"
      },
      "source": [
        "#### Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eStbMg4P0jU"
      },
      "source": [
        "1.   Define the stochastic dynamics network $f_\\theta$ and optimizer [5 pts]\n",
        "2.   Define `predict()` and `get_loss()` [5 pts]\n",
        "3.   Finish the training code, run training, run stochastic CEM [5 pts]\n",
        "3.   Conceptual question [5 pts]\n",
        "___________________________________\n",
        "Background\n",
        "\n",
        "We know trajectory optimization over learned models introduces challeneges due to distribution shift and error accumulation. We also know this problem is worse when modeling with neural nets. Let's try modeling our dynamics as a stochastic system and planning in expectation over the dynamics. We will theoretically model the state transition function as:\n",
        "\n",
        "$$\n",
        "\\mu_t, \\sigma_t = f_\\theta(s_t, a_t)\\\\\n",
        "s_{t+1} - s_t \\sim \\mathcal{N}(\\mu_t, \\sigma_t)\n",
        "$$\n",
        "\n",
        "\n",
        "Where mean $\\mu_t$ and standard deviation $\\sigma_t$ form normal distribution $\\mathcal{N}$ from which transitions are sampled. In practice we have to use these slightly less elegant equations to keep $\\sigma_t$ positive and bounded:\n",
        "\n",
        "$$\n",
        "\\mu_t, \\log \\sigma_t = f_\\theta(s_t, a_t)\\\\\n",
        "\\sigma_t = \\text{clamp} (e^{\\log \\sigma_t}, 10^{-8}, 10)  \\\\\n",
        "s_{t+1} - s_t \\sim \\mathcal{N}(\\mu_t, \\sigma_t)\n",
        "$$\n",
        "\n",
        "Where:  \n",
        "- $ \\mu_t $ is the mean of the predicted state difference\n",
        "- $ \\log \\sigma_t $ is the log of the standard deviation, modeling the uncertainty in the transition\n",
        "- $ e^{\\log \\sigma_t}$ ensures positivity\n",
        "- $\\text{clamp}()$ ensures numerical stability\n",
        "-  $\\sim$ denotes random sampling\n",
        "- $\\mathcal{N}$ denotes Normal aka Guassian Distribution\n",
        "\n",
        "___________________________________\n",
        "\n",
        "Stochastic Neural Dynamics Loss  \n",
        "\n",
        "Instead of minimizing a simple squared error loss, we now minimize the **Gaussian Negative Log-Likelihood (NLL) loss**, which accounts for both the predicted mean and uncertainty:  \n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\theta) = \\mathbb{E} \\left[ \\frac{((s_{t+1} - s_t) - \\mu_t)^2}{2\\sigma_t^2} + \\log \\sigma_t \\right]\n",
        "$$\n",
        "\n",
        "where:  \n",
        "- $ \\mathcal{L}(\\theta) $ is the stochastic dynamics loss, a function of network parameters $ \\theta $\n",
        "- $ s_{t+1} $ is the state at timestep $ t+1 $\n",
        "- $ s_t $ is the state at timestep $ t $\n",
        "- $ \\mu_t $ is the mean prediction of the state transition\n",
        "- $ \\sigma_t $ is the predicted standard deviation\n",
        "- $ \\mathbb{E} $ denotes the expectation (average over the minibatch)\n",
        "\n",
        "(hint: don't actually implement that equation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n00uTRbPp5e"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajTXsJn_VII9"
      },
      "outputs": [],
      "source": [
        "from torch.distributions import Normal\n",
        "\n",
        "n_hidden = 100\n",
        "\n",
        "class StochasticDynamics():\n",
        "    def __init__(self, n_obs, n_actions):\n",
        "        self.n_obs = n_obs\n",
        "        torch.manual_seed(0)\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(n_obs + n_actions, n_hidden),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(n_hidden, n_hidden),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(n_hidden, 2 * n_obs),  # 2 to track mean & log-variance\n",
        "        ).to(device)\n",
        "        self.loss = nn.GaussianNLLLoss()\n",
        "        # end student code\n",
        "\n",
        "    def predict(self, states, actions, sample=True):\n",
        "        x = torch.cat([states, actions], dim=-1)\n",
        "        out = self.model(x)\n",
        "        mu, log_std = out.chunk(2, dim=-1)\n",
        "        std = torch.clamp(torch.exp(log_std), 1e-8, 10)\n",
        "        if sample:  # return sampled values instead of underlying parameters\n",
        "            return torch.normal(mu, std)\n",
        "        else:\n",
        "            return mu, std\n",
        "        # end student code\n",
        "\n",
        "    def get_loss(self, states, actions, next_states):\n",
        "        mu_pred, std_pred = self.predict(states, actions, sample=False)\n",
        "        delta_states = next_states - states\n",
        "        loss = self.loss(mu_pred, delta_states, std_pred**2)\n",
        "        return loss\n",
        "        # end student code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cfnTFoBDgKkF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test passed: Stochastic dynamics predict appears correct!\n",
            "Test passed: Stochastic dynamics loss appears correct!\n"
          ]
        }
      ],
      "source": [
        "# @title Unit Tests\n",
        "\n",
        "def stoch():\n",
        "    torch.manual_seed(0)\n",
        "    # these dont match an actual rollout..\n",
        "    # print debug values during training loop rather than unit tests\n",
        "    batch_size, n_obs, n_actions = 2, 4, 1\n",
        "    s = torch.rand((batch_size, n_obs))\n",
        "    a = torch.rand((batch_size, n_actions))\n",
        "    s_ = torch.rand((batch_size, n_obs))\n",
        "\n",
        "    dyn = StochasticDynamics(4, 1)\n",
        "    torch.manual_seed(0)\n",
        "    dyn.model = nn.Linear(5, 8) # you should not use this architecture..\n",
        "\n",
        "    delta = dyn.predict(s, a)\n",
        "    # print(delta)\n",
        "    expected_delta = torch.tensor([[ 0.2525,  1.1405, -1.9571, -1.1799],\n",
        "        [-0.4345, -0.4267, -0.1200, -2.6686]])\n",
        "    assert torch.allclose(delta, expected_delta, atol=1e-4), \\\n",
        "    \"Stochastic dynamics predict does not match expected value.\"\n",
        "    print(\"Test passed: Stochastic dynamics predict appears correct!\")\n",
        "\n",
        "\n",
        "    loss = dyn.get_loss(s, a, s_)\n",
        "    # print(loss)\n",
        "    assert abs(loss.item() - (0.1352)) < 1e-4, \\\n",
        "    \"Stochastic dynamics loss does not match expected value.\"\n",
        "    print(\"Test passed: Stochastic dynamics loss appears correct!\")\n",
        "\n",
        "stoch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcGTlzR7QTp1"
      },
      "outputs": [],
      "source": [
        "writer = SummaryWriter(log_dir=f'runs/stochastic')\n",
        "\n",
        "# Params\n",
        "env = gym.make(\"InvertedPendulum-v5\")\n",
        "lr = 1e-4\n",
        "\n",
        "n_obs = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.shape[0]\n",
        "stochastic_dynamics = StochasticDynamics(n_obs, n_actions)\n",
        "optimizer = torch.optim.AdamW(stochastic_dynamics.model.parameters(), lr=lr, weight_decay=1e-2)\n",
        "stochastic_dynamics.model.train()\n",
        "\n",
        "for i in range(30_000):\n",
        "    states, actions, next_states = replay_buffer.sample()\n",
        "    \n",
        "    states = states.to(device)\n",
        "    actions = actions.to(device)\n",
        "    next_states = next_states.to(device)\n",
        "    \n",
        "    # next_states_pred = stochastic_dynamics.predict(states, actions)\n",
        "    optimizer.zero_grad()\n",
        "    loss = stochastic_dynamics.get_loss(states, actions, next_states)\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(stochastic_dynamics.model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "    # end student code\n",
        "    writer.add_scalar(\"stats/nll_loss\", loss.item(), i)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJulXk-ItcpL"
      },
      "source": [
        "You don't have to change any CEM code, but if you implemented the sampling based `predict()` function, then you're now solving the planning problem in expectation over stochastic state transitions:\n",
        "\n",
        "$$\n",
        "a_1^*, ..., a_T^* = \\arg\\max_{a_1, ..., a_T} \\mathbb{E}_{s_{t+1} \\sim p(s_{t+1} | s_t, a_t)} \\left[ \\sum_{t=1}^{T} r_t \\right]\n",
        " \\\\\n",
        "\\quad \\quad \\text{subject to} \\quad s_{t+1} - s_t \\sim \\mathcal{N}(\\mu_t, \\sigma_t)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuZ71YPdV4Vx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/lib/python3.13/site-packages/gym/wrappers/record_video.py:75: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/yunhocho/GitHub/GT-CS-8803-DRL/HWs folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "2025-11-05 18:09:30.943 Python[7113:90552729] ApplePersistenceIgnoreState: Existing state will not be touched. New state will be written to /var/folders/pd/9mrlsj5x3q1c81q4jsd0gq7c0000gn/T/org.python.python.savedState\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 4)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m cem \u001b[38;5;241m=\u001b[39m CEM(stochastic_dynamics)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mvisualize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m evaluate(cem)\n",
            "Cell \u001b[0;32mIn[5], line 30\u001b[0m, in \u001b[0;36mvisualize\u001b[0;34m(agent)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     29\u001b[0m     actions \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_action(torch\u001b[38;5;241m.\u001b[39mTensor(obs)\u001b[38;5;241m.\u001b[39mto(device)[\u001b[38;5;28;01mNone\u001b[39;00m, :])[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 30\u001b[0m obs, _, done, _\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
          ]
        }
      ],
      "source": [
        "cem = CEM(stochastic_dynamics)\n",
        "visualize(cem)\n",
        "evaluate(cem)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i080ExTee7h7"
      },
      "source": [
        "#### Conceptual Questions\n",
        "1. What type of uncertanity does our stochastic dynamics model capture? Is this the right type of uncertainty to use for modeling our system? Why or why not.<br>\n",
        "\n",
        "> Epistemic uncertainty. Yes — we want to know how confident (or uncertain) we are, not how complicated (or uncertain) the actual decision-making is. \n",
        "\n",
        "2. Did stochastic modeling help or hurt your mean duration relative to deterministic modeling on this problem? Hypothesize why. (Either outcome is fine - your answer just has to make sense.)<br>\n",
        "\n",
        "> `TODO`: run test and fill this out. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29GdWgzyEHV-"
      },
      "source": [
        "# Ensembled Dynamics [15 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVCQ59AKUOmJ"
      },
      "source": [
        "#### Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8KZJRFNUcq8"
      },
      "source": [
        "1.   Define a list of dynamics networks $f_{\\theta_j}$, a single optimizer, batch `predict()`, and `get_loss(..., j)`, finish and run training code [5 pts]\n",
        "2.   Update CEM for ensembled dynamics and run it [5 pts]\n",
        "3.   Conceptual questions [5 pts]\n",
        "________________________________________\n",
        "Background\n",
        "\n",
        "Modelling our system as stochastic doesn't capture all types of uncertainty. Let's try a different technique. Train 5 deterministic dynamics models, and plan in expectation over all of them. We will model the transition functions as:\n",
        "\n",
        "$$\n",
        "s_{t+1} - s_t = f_{\\theta_j}(s_t, a_t), \\quad \\forall j \\in \\{1, 2, 3, 4, 5\\}\n",
        "$$\n",
        "\n",
        "Where each $f_{\\theta_j}$ is independently initialized and trained on independent minibatches from $\\mathcal{D} = \\{(s, a, s')\\}$.\n",
        "\n",
        "___________________________________\n",
        "\n",
        "Ensembled Dynamics Modeling Loss:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\theta) = \\sum_{j=1}^{N} \\mathbb{E}[\\{(s_{t+1} - s_t)  - f_{\\theta_j}(s_t, a_t)\\}^2]\n",
        "$$\n",
        "\n",
        "Where $ (s, a, s') $ are sampled independently from $ \\mathcal{D}$ for each network $f_{\\theta_j}$.\n",
        "\n",
        "\n",
        "And:\n",
        "- $ \\mathcal{L} $ is the total ensemble loss; a function of all $j$ network parameters $\\theta_j$\n",
        "- $N$ is the number of networks in the ensemble\n",
        "-$s_{t+1}$ is state at timestep $t+1$\n",
        "-$s_t$ is action at timestep $t$\n",
        "-$f_{\\theta_j}(s_t, a_t)$ is the $j^{th}$ dynamics network parametrized by $\\theta_j$\n",
        "-$\\mathbb{E}$ is the expectation or average over the minibatch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23IJf81uUOSY"
      },
      "source": [
        "#### Code\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uigT0cIFrUDP"
      },
      "outputs": [],
      "source": [
        "n_hidden = 100\n",
        "\n",
        "class EnsembleDynamics():\n",
        "    def __init__(self, n_obs, n_actions):\n",
        "        torch.manual_seed(0)\n",
        "        self.N = 5\n",
        "        self.n_obs = n_obs\n",
        "        self.models = [nn.Sequential(\n",
        "            nn.Linear(n_obs + n_actions, n_hidden),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(n_hidden, n_hidden),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(n_hidden, n_obs),\n",
        "        ).to(device)] * self.N\n",
        "        # end student code\n",
        "\n",
        "    # first dimension of states and return should be N\n",
        "    def predict(self, states, actions):\n",
        "        if states.dim() == 2:  # non-ensembled states\n",
        "            states = states.unsqueeze(0)  # add singleton 'N' dim (for dim consistency @ torch.cat)\n",
        "        n_batch = states.shape[1]\n",
        "        delta_states = torch.zeros((self.N, n_batch, self.n_obs))\n",
        "        for i, state in enumerate(states):\n",
        "            x = torch.cat([state, actions], dim=-1)\n",
        "            delta_states[i,...] = self.models[i](x)\n",
        "        return delta_states\n",
        "        # end student code\n",
        "\n",
        "    def get_loss(self, states, actions, next_states, j):\n",
        "        delta_states_pred = self.predict(states, actions)\n",
        "        delta_states = next_states - states\n",
        "        \n",
        "        # loss = 0\n",
        "        # for i, delta_state_pred in enumerate(delta_states_pred):\n",
        "        #     loss += F.mse_loss(delta_state_pred, delta_states)\n",
        "        loss = F.mse_loss(delta_states_pred[j], delta_states)  # unit test success\n",
        "        # loss = F.mse_loss(delta_states_pred[j], delta_states)  # suppresses accuracy warning from torch\n",
        "        return loss # loss of jth model\n",
        "        # end student code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6E5fQdSQgUbY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test passed: Ensemble dynamics predict appears correct!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/pd/9mrlsj5x3q1c81q4jsd0gq7c0000gn/T/ipykernel_7113/2423778404.py:36: UserWarning: Using a target size (torch.Size([4])) that is different to the input size (torch.Size([2, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  loss = F.mse_loss(delta_states_pred[j], delta_states[j])\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "Ensemble dynamics loss does not match expected value.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[53], line 43\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m-\u001b[39m (\u001b[38;5;241m0.4017\u001b[39m)) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-4\u001b[39m, \\\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnsemble dynamics loss does not match expected value.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest passed: Ensemble dynamics loss appears correct!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m \u001b[43mensemb\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[53], line 39\u001b[0m, in \u001b[0;36mensemb\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m loss \u001b[38;5;241m=\u001b[39m dyn\u001b[38;5;241m.\u001b[39mget_loss(s[\u001b[38;5;241m0\u001b[39m], a, s_[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# print(loss)\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m-\u001b[39m (\u001b[38;5;241m0.4017\u001b[39m)) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-4\u001b[39m, \\\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnsemble dynamics loss does not match expected value.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest passed: Ensemble dynamics loss appears correct!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mAssertionError\u001b[0m: Ensemble dynamics loss does not match expected value."
          ]
        }
      ],
      "source": [
        "# @title Unit Tests (Non Exhaustive)\n",
        "\n",
        "def ensemb():\n",
        "    torch.manual_seed(0)\n",
        "    # these dont match an actual rollout..\n",
        "    # print debug values during training loop rather than unit tests\n",
        "    batch_size, n_obs, n_actions, N = 2, 4, 1, 5\n",
        "    s = torch.rand((N, batch_size, n_obs))\n",
        "    a = torch.rand((batch_size, n_actions))\n",
        "    s_ = torch.rand((N, batch_size, n_obs))\n",
        "\n",
        "    dyn = EnsembleDynamics(4,1)\n",
        "    torch.manual_seed(0)\n",
        "    dyn.models = [nn.Linear(5, 4)]*5 # you should not do this ..\n",
        "\n",
        "    delta = dyn.predict(s, a)\n",
        "    # print(delta)\n",
        "    expected_delta = torch.tensor([[[ 0.1784,  0.5125, -0.4006,  0.3599],\n",
        "         [-0.2039,  0.5696, -0.7091,  0.2641]],\n",
        "\n",
        "        [[-0.0387,  0.5905, -0.5743,  0.3233],\n",
        "         [-0.1179,  0.4850, -0.4340,  0.2391]],\n",
        "\n",
        "        [[ 0.1092,  0.5561, -0.5061,  0.3428],\n",
        "         [-0.0961,  0.5799, -0.7381,  0.3018]],\n",
        "\n",
        "        [[-0.1596,  0.8154, -0.7121,  0.5816],\n",
        "         [-0.2096,  0.4903, -0.6012,  0.1556]],\n",
        "\n",
        "        [[ 0.0700,  0.5045, -0.3103,  0.3136],\n",
        "         [-0.1676,  0.6439, -0.7756,  0.4240]]])\n",
        "    assert torch.allclose(delta, expected_delta, atol=1e-4), \\\n",
        "    \"Ensemble dynamics predict does not match expected value.\"\n",
        "    print(\"Test passed: Ensemble dynamics predict appears correct!\")\n",
        "\n",
        "\n",
        "    loss = dyn.get_loss(s[0], a, s_[0], 0)\n",
        "    # print(loss)\n",
        "    assert abs(loss.item() - (0.4017)) < 1e-4, \\\n",
        "    \"Ensemble dynamics loss does not match expected value.\"\n",
        "    print(\"Test passed: Ensemble dynamics loss appears correct!\")\n",
        "\n",
        "ensemb()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YK5qlmm5rcyv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/pd/9mrlsj5x3q1c81q4jsd0gq7c0000gn/T/ipykernel_7113/1564472666.py:24: UserWarning: `parameters` is an empty generator, no gradient clipping will occur.\n",
            "  torch.nn.utils.clip_grad_norm_(weights[i], 1.0)  # optional\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[54], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# next_state_pred = ensemble_dynamics.predict()\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 21\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mensemble_dynamics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     24\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(weights[i], \u001b[38;5;241m1.0\u001b[39m)  \u001b[38;5;66;03m# optional\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[52], line 30\u001b[0m, in \u001b[0;36mEnsembleDynamics.get_loss\u001b[0;34m(self, states, actions, next_states, j)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, states, actions, next_states, j):\n\u001b[0;32m---> 30\u001b[0m     delta_states_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     delta_states \u001b[38;5;241m=\u001b[39m next_states \u001b[38;5;241m-\u001b[39m states\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# loss = 0\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# for i, delta_state_pred in enumerate(delta_states_pred):\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m#     loss += F.mse_loss(delta_state_pred, delta_states)\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[52], line 25\u001b[0m, in \u001b[0;36mEnsembleDynamics.predict\u001b[0;34m(self, states, actions)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, state \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(states):\n\u001b[1;32m     24\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([state, actions], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m     delta_states[i,\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m delta_states\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/torch/nn/modules/container.py:244\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 244\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "writer = SummaryWriter(log_dir=f'runs/ensembled')\n",
        "\n",
        "# Params\n",
        "env = gym.make(\"InvertedPendulum-v5\")\n",
        "lr = 1e-4\n",
        "N = 5\n",
        "\n",
        "n_obs = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.shape[0]\n",
        "ensemble_dynamics = EnsembleDynamics(n_obs, n_actions)\n",
        "weights = [model.parameters() for model in ensemble_dynamics.models]\n",
        "optimizers = [torch.optim.AdamW(weight, lr=lr, weight_decay=2e-3) for weight in weights]\n",
        "[model.train() for model in ensemble_dynamics.models]\n",
        "\n",
        "for i in range(30_000):\n",
        "    states, actions, next_states = replay_buffer.sample()\n",
        "    states = states.repeat(N, 1, 1)\n",
        "\n",
        "    states = states.to(device)\n",
        "    actions = actions.to(device)\n",
        "    next_states = next_states.to(device)\n",
        "\n",
        "    for i in range(N):\n",
        "        # next_state_pred = ensemble_dynamics.predict()\n",
        "        optimizer.zero_grad()\n",
        "        loss = ensemble_dynamics.get_loss(states, actions, next_states, j=i)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(weights[i], 1.0)  # optional\n",
        "        optimizers[i].step()\n",
        "\n",
        "    # end student code\n",
        "    writer.add_scalar(\"stats/mse_loss\", loss.item() / ensemble_dynamics.N, i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVZr5Lsd8uat"
      },
      "source": [
        "Update your CEM implementation below so it solves the following constrained optimization in expectation over the $N$ models in your ensemble:\n",
        "\n",
        "\n",
        "\\begin{aligned}\n",
        "a_1^*, ..., a_T^* &= \\arg\\max_{a_1, ..., a_T} \\frac{1}{N} \\sum_{j=1}^{N} \\sum_{t=1}^{T} r_t^{(j)} \\\\\n",
        "\\\\\n",
        "\\text{subject to} \\quad s_{t+1}^{(j)} &= s_t^{(j)} + f_{\\theta_j}(s_t^{(j)}, a_t), \\quad \\forall j \\in \\{1, ..., N\\} \\\\\n",
        "\\\\\n",
        "\\text{given} \\quad s_0^{(j)} &= s_0, \\quad \\forall j \\in \\{1, ..., N\\}\n",
        "\\end{aligned}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFNgOgbLxJ8q"
      },
      "outputs": [],
      "source": [
        "# student code here\n",
        "\n",
        "class CEM:\n",
        "    def __init__(self, dynamics):\n",
        "        self.dynamics: EnsembleDynamics = dynamics\n",
        "        self.T = 10  # planning time horizon\n",
        "        self.n_actions = 1\n",
        "        self.cem_iter = 20\n",
        "        self.n_samples = 1000\n",
        "        self.n_elite = 200\n",
        "\n",
        "    def compute_rewards(self, state):\n",
        "        reward_t = -state[..., 0].square() / 5.  # incremental position penalty\n",
        "        reward_t -= state[..., 1].square()  # incremental angle penalty\n",
        "        # angle and position limits\n",
        "        soft_done = torch.logical_or(state[..., 1].abs() > 0.2, state[..., 0].abs() > 1.)\n",
        "        reward_t +=  1 - 2 * soft_done.float()\n",
        "        return reward_t\n",
        "\n",
        "    def get_action(self, initial_states):\n",
        "        mean = torch.zeros((self.T, self.n_actions))\n",
        "        std_dev = torch.ones((self.T, self.n_actions))\n",
        "\n",
        "        for i in range(self.cem_iter):\n",
        "            # initial random rollout (before CEM)\n",
        "            states = initial_states.repeat(self.n_samples, 1)\n",
        "            actions = Normal(mean, std_dev).sample((self.n_samples,)).clamp(-3, 3).to(device)\n",
        "            returns = torch.zeros((self.n_samples,))\n",
        "\n",
        "            # roll out actions in batch (for T steps)\n",
        "            for j in range(self.T):  # using for loop since chronological dependency and low loop repeat n\n",
        "                with torch.no_grad():\n",
        "                    delta_states_pred = self.dynamics.predict(states, actions[:,j,...])  # sa: state & action\n",
        "                    next_states_pred = states + delta_states_pred\n",
        "                states = next_states_pred  # alias (for loop update)\n",
        "                returns += self.compute_rewards(states)\n",
        "\n",
        "            # choose elites & re-fit\n",
        "            returns_sorted, idx_sorted = returns.sort(dim=0, descending=True)\n",
        "            elite_actions = actions[idx_sorted[:self.n_elite], ...]\n",
        "            elite_std, elite_mean = torch.std_mean(elite_actions, dim=0, unbiased=False)\n",
        "            mean, std_dev = elite_mean, elite_std  # alias (for loop update)\n",
        "            std_dev = std_dev.clamp_min(0.5)  # min_clamp std to ensure exploration\n",
        "\n",
        "        # max-pool action wrt reward from last distribution\n",
        "        # return actions[idx_sorted[0], 0, ...].unsqueeze(-1)\n",
        "        \n",
        "        # mean-pool action wrt reward from final elite distribution\n",
        "        # NOTE: the closest to the unit test's expected value... altho there's numerical differences for some reason\n",
        "        return mean[0].view(1, self.n_actions)\n",
        "        \n",
        "        # sample from final optimized distribution\n",
        "        # NOTE: why doesn't this work?!\n",
        "        action = Normal(mean, std_dev).sample((1,)).clamp(-3, 3).to(device)\n",
        "        return action[:,0,...]\n",
        "        # end student code\n",
        "\n",
        "# end student code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dzdofMVigSH6"
      },
      "outputs": [],
      "source": [
        "# @title Unit Tests (Non Exhaustive)\n",
        "\n",
        "def ensemble_cem():\n",
        "    torch.manual_seed(0)\n",
        "    # these dont match an actual rollout..\n",
        "    # print debug values during training loop rather than unit tests\n",
        "    batch_size, n_obs = 1, 4\n",
        "    s = 2*torch.rand((batch_size, n_obs)).to(device)-3\n",
        "\n",
        "    dyn = EnsembleDynamics(4, 1)\n",
        "    torch.manual_seed(0)\n",
        "    dyn.models = [nn.Linear(5, 4).to(device)]*5 # you should not do this ..\n",
        "    cem = CEM(dyn)\n",
        "    a = cem.get_action(s)\n",
        "\n",
        "    assert a.shape == torch.Size([1, 1]), \\\n",
        "    \"Action size appears wrong\"\n",
        "\n",
        "    # print(a.item())\n",
        "    if device == 'cpu':\n",
        "        assert abs(a.item() -  (-1.4559649229049683)) < 1e-4, \\\n",
        "        \"Action value appears wrong\"\n",
        "    else: #todo fix this...\n",
        "        assert abs(a.item() -  (-1.4642404317855835)) < 1e-4, \\\n",
        "        \"Action value appears wrong\"\n",
        "    print(\"Test passed: CEM Action appears correct!\")\n",
        "\n",
        "ensemble_cem()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jBEquIRx-lS"
      },
      "outputs": [],
      "source": [
        "cem = CEM(ensemble_dynamics)\n",
        "visualize(cem)\n",
        "evaluate(cem)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWKbjYR9fqrO"
      },
      "source": [
        "#### Conceptual Question\n",
        "\n",
        "1. What type of uncertanity does our ensembled dynamics model capture? Is this the right type of uncertainty for our problem? Why or why not.<br>\n",
        "\n",
        "> The ensembled dynamics model captures the epistemic uncertainty—when the individual dynamics models agree, it signals confidence, and when they don't agree, it signals uncertainty. Our problem is fully observable (there are no hidden states) so it's not strictly required for this specific cartpole task, but it is effective for dealing with distribution (covariance) shifts. \n",
        "\n",
        "2. Did ensembled modeling help or hurt your mean duration relative to deterministic modeling on this problem? Hypothesize why. (Either outcome is fine - your answer just has to make sense.)<br>\n",
        "\n",
        "> `TODO`: check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "8PHhuXXvnxb8",
        "bPnPio6d94fy",
        "Ek7oKfhX_Umm",
        "MV-bVS5d-F6C",
        "BTGB_0vkIpmx",
        "aIjmwKUuIxFl",
        "tQj9reBb_tV7",
        "AZQDZHPrPs9E",
        "jVErJJ_cRzbu",
        "yRf3Kl0sERdX",
        "JTXy-sDbPnu7",
        "3n00uTRbPp5e",
        "29GdWgzyEHV-",
        "kVCQ59AKUOmJ",
        "23IJf81uUOSY"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "mjp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-odEg64EKVXO"
   },
   "source": [
    "# HW 1 - Policy Gradients & Proximal Policy Optimization\n",
    "This assignment builds to a simple PPO (2017) implementations by progressing from PPOs predecessor algorithms: <br> REINFORCE (\\~1992) and Vanilla Policy Gradients (\\~1999). Note, many variations of these algorithms exist. Please use the math contained in this notebook for the coding sections.\n",
    "\n",
    "It's recommended that you use [Google Colab](https://colab.research.google.com/) for this assignment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YkZLMuXu7Fm"
   },
   "source": [
    "## 0. Warm Up Questions [24 pts total]\n",
    "Answer each question concisely. One sentence, one formula, one line of code, etc. Use of $\\LaTeX$ formatting for math is encouraged.\n",
    "\n",
    "1.    What is an MDP and what are its four main parts? <br> [1 point]\n",
    "\n",
    "The Markov Decision Process is a mathematical formulation of stateful, time-dependent phenomena. Its main components are: state (or observation), action, *dynamics* (state transition probability) and reward.\n",
    "\n",
    "2.    What is the markov property? <br> [1 point]\n",
    "\n",
    "The future only depends on the present, not the past. In other words, the environment is \"memoryless\".\n",
    "\n",
    "3.    What is the formula for the objective aka sum of discounted rewards? <br> [1 point]\n",
    "\n",
    "$J = E[\\sum^{N}_{i=0} \\gamma^i r_i]$\n",
    "\n",
    "4.    Complete the sentence: 'Policy gradient' is shorthand for the 'gradient of ??? with respect to ???'. <br> [1 point]\n",
    "\n",
    "reward, policy (or its weights)\n",
    "\n",
    "5.    What does $\\nabla _\\theta J (\\theta)$ mean in basic language? <br> [1 point]\n",
    "\n",
    "The \"direction\" of change of $\\theta$ that would produce the greatest change in the reward—the direction of steepest ascent.\n",
    "\n",
    "6.    What is the formula for gradient of the objective in REINFORCE? (policy gradient slides - Canvas/files/lec-4) [2 points]\n",
    "\n",
    "$$ \\nabla _\\theta J (\\theta) \\approx  \\int \\nabla_\\theta p_\\theta (\\tau) r (\\tau) d\\tau = \\int p_\\theta (\\tau) \\nabla_\\theta log p_\\theta (\\tau) r (\\tau) d\\tau = E_{\\tau ~ p_\\theta (\\tau)}[\\nabla_\\theta log p_\\theta (\\tau) r (\\tau)]$$\n",
    "\n",
    "\n",
    "7.    Does subtracting a baseline from returns introduce bias in expectation? (policy gradient slides - Canvas/files/lec-4) <br> [2 points]\n",
    "\n",
    "No, it is unbiased.\n",
    "\n",
    "8.    Do on-policy algorithms use a replay buffer? <br> [1 point]\n",
    "\n",
    "No. They strictly rely on data obtained from the latest policy (hence, \"on\" policy)\n",
    "\n",
    "9.    What does $\\pi _\\theta (a_t | s_t)$ mean in basic language? <br> [2 points]\n",
    "\n",
    "The probability of a policy (parameterized by theta) choosing an action given a state.\n",
    "\n",
    "In plain language: how likely that specific action is chosen by a RL model (defined by model weights $\\theta$) in a specific situation called $s_t$\n",
    "\n",
    "10.    What is the log prob of getting heads when flipping a coin? Final answer should be a numerical value. <br> [2 points]\n",
    "\n",
    "$log(0.5) = -0.693$\n",
    "\n",
    "11.    Finish this basic property of logs: [2 points]\n",
    "$\\frac{A}{B} = \\exp (\\log A - \\log B)$\n",
    "\n",
    "12. What is a logit in the DRL context? <br> [2 points]\n",
    "\n",
    "\"Raw score\" output by a neural network. This is not a probability, because the sum of logits typically do not add up to 1 — i.e., it is not a proper probability distribution. Typicaly, this is fed into a softmax function to get the normalized probability scores. \n",
    "\n",
    "~~The log probability of taking a certain action—likely from the action-value ($Q$) function, or directly from a policy. The logit value exists for every possible action, and an `argmax` operator would be applied to select the action with the highest expected cumulative return.~~\n",
    "\n",
    "13. Is a Categorical Distribution continuous or discrete? <br> [2 points]\n",
    "\n",
    "Discrete. \n",
    "\n",
    "You cannot be half a category and half another category at the same time. You either are, or are not—there is no \"middle ground\" category.\n",
    "\n",
    "14. Logits are used to construct a Categorical distribution. Finish the code to get the log probability of the actions that were sampled. Hint: https://pytorch.org/docs/stable/distributions.html [2 points]\n",
    "\n",
    "        logits = self.policy(obs)\n",
    "        probs = categorical.Categorical(logits=logits)\n",
    "        actions = probs.sample()\n",
    "        log_probs = probs.log_prob(actions)\n",
    "\n",
    "\n",
    "15. In [CartPole-v1](https://www.gymlibrary.dev/environments/classic_control/cart_pole/) what are the physical meanings of states and actions and are they discrete or continuous? [2 points]\n",
    "\n",
    "States:\n",
    "- Position (x-axis): continuous\n",
    "- Velocity (x-axis): continuous\n",
    "- Angle (z-axis): continuous\n",
    "- Angular velocity (z-axis): continuous\n",
    "\n",
    "Actions:\n",
    "- Go left [0] or go right [1] (x-axis): discrete; possible values: [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2ltUIFtSIRo"
   },
   "source": [
    "## Imports and Set up\n",
    "Installs gymnasium, imports deep learning libs, sets torch device. **You shouldn't need to change this code.** Your Colab runtime should default to CPU. To double check: **click Runtime (top left of notebook) -> Change runtime type -> select a CPU -> Save**. For simplicity, this notebook doesn't manage data transfers between CPU and GPU. You need to use CPU runtime for it to work unmodified. Feel free to experiment with GPUs after submitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "j4bFtFg6LITX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# !pip install gymnasium\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.distributions import categorical\n",
    "from copy import deepcopy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# random seeds for reproducability\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "random.seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "3LnvUng8oQsQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed: Device is CPU.\n"
     ]
    }
   ],
   "source": [
    "# @title Device check\n",
    "def test_device_is_cpu():\n",
    "    assert device.type == \"cpu\", (\n",
    "        \"Test failed: Device is not CPU! Read Imports and Set up\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Run the test\n",
    "test_device_is_cpu()\n",
    "print(\"Test passed: Device is CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2Cd4Pf9SkM9"
   },
   "source": [
    "## Trajectory Data Storage\n",
    "This is boilerplate code that lets your on-policy algorithms store their interactions with the environment. It also calculates returns as the sum of discounted rewards\n",
    "$\n",
    "R = \\sum_{t=0}^{T} \\gamma^t r_t\n",
    "$.\n",
    "Note, when a terminal condition is reached (not_dones = False), the sum resets to 0. More sopisticated PPO implementations use GAE, but it will work without it. **You shouldn't need to change this code,** but you should understand the `store()` and `calc_returns()` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "65bvpyHhL7Jw"
   },
   "outputs": [],
   "source": [
    "class TrajData:\n",
    "    def __init__(self, n_steps, n_envs, n_obs, n_actions):\n",
    "        s, e, o, a = n_steps, n_envs, n_obs, n_actions\n",
    "        from torch import zeros\n",
    "\n",
    "        self.states = zeros((s, e, o))\n",
    "        self.actions = zeros((s, e))\n",
    "        self.rewards = zeros((s, e))\n",
    "        self.not_dones = zeros((s, e))\n",
    "\n",
    "        self.log_probs = zeros((s, e))\n",
    "        self.returns = zeros((s, e))\n",
    "\n",
    "        self.n_steps = s\n",
    "\n",
    "    def detach(self):\n",
    "        self.actions = self.actions.detach()\n",
    "        self.log_probs = self.log_probs.detach()\n",
    "\n",
    "    def store(self, t, s, a, r, lp, d):\n",
    "        self.states[t] = s\n",
    "        self.actions[t] = a\n",
    "        self.rewards[t] = torch.Tensor(r)\n",
    "\n",
    "        self.log_probs[t] = lp\n",
    "        self.not_dones[t] = 1 - torch.Tensor(d)\n",
    "\n",
    "    def calc_returns(self, gamma=0.99):\n",
    "        self.returns = deepcopy(self.rewards)\n",
    "\n",
    "        for t in reversed(range(self.n_steps - 1)):\n",
    "            self.returns[t] += self.returns[t + 1] * self.not_dones[t] * gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3I2uUuD3YS_I"
   },
   "source": [
    "## DRL Rollout and Update Loop\n",
    "This is more boilerplate code. It instantiates your parallel gym environments, neural nets (which you will define next), optimizer, and tensorboard logging. It also establishes the rollout/update cycle. During rollout, the agent collects $(s, a, r)$ tuples from the environment. During update, losses are calculated and the DRL agent is updated via gradient descent. **You shouldn't need to change this code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yOmLqtPSL9E_"
   },
   "outputs": [],
   "source": [
    "class DRL:\n",
    "    def __init__(self):\n",
    "        self.n_envs = 64\n",
    "        self.n_steps = 256\n",
    "        self.n_obs = 4\n",
    "\n",
    "        self.envs = gym.vector.SyncVectorEnv(\n",
    "            [lambda: gym.make(\"CartPole-v1\") for _ in range(self.n_envs)]\n",
    "        )\n",
    "\n",
    "        self.traj_data = TrajData(\n",
    "            self.n_steps, self.n_envs, self.n_obs, n_actions=1\n",
    "        )  # 1 action choice is made\n",
    "        self.agent = Agent(self.n_obs, n_actions=2)  # 2 action choices are available\n",
    "        self.optimizer = Adam(self.agent.parameters(), lr=1e-3)\n",
    "        self.writer = SummaryWriter(log_dir=f\"runs/{self.agent.name}\")\n",
    "\n",
    "    def rollout(self, i):\n",
    "        obs, _ = self.envs.reset()\n",
    "        obs = torch.Tensor(obs)\n",
    "\n",
    "        for t in range(self.n_steps):\n",
    "            # PPO doesn't use gradients here, but REINFORCE and VPG do.\n",
    "            with torch.no_grad() if self.agent.name == \"PPO\" else torch.enable_grad():\n",
    "                actions, probs = self.agent.get_action(obs)\n",
    "            log_probs = probs.log_prob(actions)\n",
    "            next_obs, rewards, done, truncated, infos = self.envs.step(actions.numpy())\n",
    "            done = done | truncated  # episode doesn't truncate till t = 500, so never\n",
    "            self.traj_data.store(t, obs, actions, rewards, log_probs, done)\n",
    "            obs = torch.Tensor(next_obs)\n",
    "\n",
    "        self.traj_data.calc_returns()\n",
    "\n",
    "        self.writer.add_scalar(\"Reward\", self.traj_data.rewards.mean(), i)\n",
    "        self.writer.flush()\n",
    "\n",
    "    def update(self):\n",
    "        # A primary benefit of PPO is that it can train for\n",
    "        # many epochs on 1 rollout without going unstable\n",
    "        epochs = 10 if self.agent.name == \"PPO\" else 1\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            loss = self.agent.get_loss(self.traj_data)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        self.traj_data.detach()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTWYOKF-bxFa"
   },
   "source": [
    "## Tensorboard\n",
    "This will launch an interactive tensorboard window within collab. It will display rewards in (close to) real time while your agents are training. You'll likely have to refresh if its not updating (circular arrow to right in the orange bar). **You shouldn't need to change this code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5iKKG2ZwM9hW"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-629f6fbed82c07cd\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-629f6fbed82c07cd\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Launch TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "id": "RKOqilTFO_t9"
   },
   "outputs": [],
   "source": [
    "# @title Visualization code. Used later.\n",
    "\n",
    "import os\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from IPython.display import Video, display, clear_output\n",
    "\n",
    "\n",
    "def visualize(agent):\n",
    "    video_dir = \"./videos\"  # Directory to save videos\n",
    "    os.makedirs(video_dir, exist_ok=True)\n",
    "\n",
    "    # Create environment with proper render_mode\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "    # Apply video recording wrapper\n",
    "    env = RecordVideo(env, video_folder=video_dir, episode_trigger=lambda x: True)\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    for t in range(4096):\n",
    "        actions, _ = agent.get_action(\n",
    "            torch.Tensor(obs)[None, :]\n",
    "        )  # Get action from policy\n",
    "        obs, _, done, _ = env.step(actions.cpu().item())\n",
    "\n",
    "        if done:\n",
    "            # self.writer.add_scalar(\"Duration\", t, i)\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    # Display the latest video\n",
    "    video_path = os.path.join(\n",
    "        video_dir, sorted(os.listdir(video_dir))[-1]\n",
    "    )  # Get the latest video\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    display(Video(video_path, embed=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hs6mPduOcQ0T"
   },
   "source": [
    "--------------------------------------------------------------------------------\n",
    "## 1. REINFORCE [30 pts]\n",
    "1.   Define your policy network [10 pts]\n",
    "2.   Define the reinforce policy loss using rollout data stored in traj_data [15 pts]\n",
    "3.   Conceptual question [5 pts]\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "HINTS:\n",
    "\n",
    "If you're not super familar with defining networks in pytorch, check out this [tutorial](https://medium.com/writeasilearn/using-sequential-module-to-build-a-neural-network-a34ca3f37203).\n",
    "\n",
    "#### Policy loss for REINFORCE:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = -\\frac{1}{N \\cdot T} \\sum_{i=0}^N \\sum_{t=0}^T \\log \\pi_\\theta(a_{i,t} | s_{i,t}) \\cdot R_{i,t}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\mathcal{L} $ is the policy loss; a function of network parameters $\\theta$\n",
    "- $N$ is the total number of environments\n",
    "- $T$ is the total number of time steps (the slides don't divide by $T$, but it doesnt change the gradient, and you need it to pass the unit tests)\n",
    "- $ \\log \\pi_{\\theta}(a_{i, t} | s_{i,t}) $ is the logarithm of the probability of the action $a$ that was taken in state $s$, given policy $\\pi$ parametrized by $\\theta$, at timestep t in environment i\n",
    "- $ R_{i,t} $ is the return (sum of discounted rewards) for environment i at timestep t\n",
    "\n",
    "<br>\n",
    "\n",
    "For simplicity, expectation notation is often used, and the subscript $i$ is often dropped:\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = -\\mathbb{E}\\left[ \\log \\pi_{\\theta}(a_t | s_t) \\cdot R_t \\right]\n",
    "$$\n",
    "\n",
    "We follow this convention going forward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "z_a2Ji6KLpYo"
   },
   "outputs": [],
   "source": [
    "HIDDEN_WIDTH = 4\n",
    "# HIDDEN_DEPTH = 0\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, n_obs, n_actions):  # use these\n",
    "        super().__init__()\n",
    "        self.name = \"REINFORCE\"\n",
    "\n",
    "        torch.manual_seed(0)  # needed before policy init for fair comparison\n",
    "\n",
    "        # todo: student code here\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(n_obs, HIDDEN_WIDTH),\n",
    "            nn.Linear(HIDDEN_WIDTH, HIDDEN_WIDTH),\n",
    "            nn.Linear(HIDDEN_WIDTH, n_actions),\n",
    "        )  # replace\n",
    "        # end student code\n",
    "\n",
    "    def get_loss(self, traj_data: TrajData):\n",
    "        # todo: student code here\n",
    "        policy_loss = torch.mean(-traj_data.log_probs * traj_data.returns)  # replace\n",
    "        # end student code\n",
    "        return policy_loss\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        logits = self.policy(obs)\n",
    "        probs = categorical.Categorical(logits=logits)\n",
    "        actions = probs.sample()\n",
    "        return actions, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "form",
    "id": "T884A8vHUZwz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed: REINFORCE policy appears correct!\n",
      "Test passed: REINFORCE loss appears correct!\n"
     ]
    }
   ],
   "source": [
    "# @title REINFORCE Unit Tests (must run REINFORCE Agent cell above first)\n",
    "def REINFORCE_policy():\n",
    "    a = Agent(16, 4)\n",
    "    assert (\n",
    "        a.name == \"REINFORCE\"\n",
    "        and a.policy(torch.randn(8, 16)).shape == (8, 4)\n",
    "        and isinstance(list(a.policy.children())[-1], nn.Linear)\n",
    "    ), f\"Network not initialized correctly\"\n",
    "    print(\"Test passed: REINFORCE policy appears correct!\")\n",
    "\n",
    "\n",
    "REINFORCE_policy()\n",
    "\n",
    "\n",
    "def REINFORCE_loss():\n",
    "    n_steps, n_envs, n_obs, n_actions = 10, 5, 4, 1\n",
    "    traj_data = TrajData(n_steps, n_envs, n_obs, n_actions)\n",
    "    torch.manual_seed(0)\n",
    "    traj_data.states = torch.rand_like(traj_data.states)\n",
    "    traj_data.actions = torch.randint(0, n_actions, traj_data.actions.shape)\n",
    "    traj_data.rewards = torch.rand_like(traj_data.rewards)\n",
    "    traj_data.not_dones = torch.randint(0, 2, traj_data.not_dones.shape)\n",
    "    traj_data.log_probs = torch.rand_like(traj_data.log_probs)\n",
    "    traj_data.returns = torch.rand_like(traj_data.returns)\n",
    "    a = Agent(n_obs=n_obs, n_actions=n_actions)\n",
    "    assert abs(a.get_loss(traj_data).item() - (-0.2369)) < 1e-4, (\n",
    "        \"REINFORCE loss does not match expected value.\"\n",
    "    )\n",
    "    print(\"Test passed: REINFORCE loss appears correct!\")\n",
    "\n",
    "\n",
    "REINFORCE_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvZxxF6TLrP0"
   },
   "source": [
    "Run the REINFORCE Agent cell above, and then run the rollout/update cell below. <br> Scroll back up to tensorboard and refresh (circular white arrow in the right of the orange bar) to visualize your reward curve.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BleZWK8yMFyp"
   },
   "outputs": [],
   "source": [
    "drl = DRL()\n",
    "for i in range(250):\n",
    "    drl.rollout(i)\n",
    "    drl.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4XQsYbKC3qas"
   },
   "source": [
    "#### REINFORCE Conceptual question:\n",
    "In 1 or 2 sentences, how does minimizing the REINFORCE loss above achieve our RL goal? <br> Hint: (policy gradient slides - Canvas/files/lec-4 - \"What did we just do?\")<br>\n",
    "\n",
    "> The REINFORCE loss is just the reward, with a negative sign — i.e., the greater the reward, the lesser the loss. The reward itself is calculated by the environment (CartPole), and REINFORCE does the job of finding a good set of parameters $\\theta$ that tend to choose actions that achieve a large value of reward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-P2zM8Bnc6d"
   },
   "source": [
    "--------------------------------------------------------------------------------\n",
    "## 2. Vanilla Policy Gradient (aka REINFORCE with Baseline)[30 pts]\n",
    "\n",
    "1.   Define your networks [10 pts]\n",
    "  *   Value network\n",
    "  *   Policy network (same as before)\n",
    "\n",
    "\n",
    "2.   Define your losses [15 pts]\n",
    "  *   Value loss\n",
    "  *   Policy loss (similar to before)\n",
    "  *   Add them\n",
    "\n",
    "3.   Conceptual question [5 pts]\n",
    "--------------------------------------------------------------------------------\n",
    "HINTS:\n",
    "\n",
    "#### Value loss\n",
    "Mean Squared Error (MSE) between the experienced returns and predicted value:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{value}}(\\theta) = \\mathbb{E}\\left[ (R_t - V_{\\theta}(s_t))^2 \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\mathcal{L}_{\\text{value}}(\\theta) $ is the value network loss\n",
    "- $ V_{\\theta}(s_t) $ is the predicted value for state $ s_t $ from the value network\n",
    "- $ R_t $ is the return (sum of discounted rewards)\n",
    "\n",
    "\n",
    "#### Policy Loss\n",
    "\n",
    "The VPG policy loss is quite similar to REINFORCE, but rather than using returns, we use **returns minus a baseline value prediction**. This quantity is known as the advantage $A(s_t, a_t)$. The advantage is usually defined as $A(s_t, a_t) = Q(s_t, a_t) - V(s_t)$. Returns act as the Q function in our case.\n",
    "\n",
    "$$A(s_t, a_t) = R_t - V_{\\theta}(s_t)$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{policy}}(\\theta) = - \\mathbb{E}\\left[ \\log \\pi_{\\theta}(a_t | s_t) \\cdot A(s_t, a_t) \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\mathcal{L}_{\\text{policy}}(\\theta) $ is the policy loss\n",
    "- $ \\log \\pi_{\\theta}(a_t | s_t) $ is the logarithm of the probability of the action $a_t$ that was taken in state $s_t$\n",
    "- $A(s_t, a_t)$ is the advantage of action $a_t$ that was taken in $s_t$, campared to the average for state $s_t$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "A8GEi3NvqUTD"
   },
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, n_obs, n_actions):  # use these\n",
    "        super().__init__()\n",
    "        self.name = \"VPG\"\n",
    "\n",
    "        torch.manual_seed(0)  # needed before network init for fair comparison\n",
    "\n",
    "        # todo: student code here\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(n_obs, HIDDEN_WIDTH),\n",
    "            nn.Linear(HIDDEN_WIDTH, HIDDEN_WIDTH),\n",
    "            nn.Linear(HIDDEN_WIDTH, n_actions),\n",
    "        )  # replace\n",
    "\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(n_obs, HIDDEN_WIDTH),\n",
    "            nn.Linear(HIDDEN_WIDTH, HIDDEN_WIDTH),\n",
    "            nn.Linear(HIDDEN_WIDTH, 1),\n",
    "        )  # replace\n",
    "        # end student code\n",
    "\n",
    "    def get_loss(self, traj_data: TrajData):\n",
    "        # todo: student code here\n",
    "        value = self.value(traj_data.states).squeeze(-1)\n",
    "        advantage = traj_data.returns - value\n",
    "        value_loss = torch.mean(advantage**2)\n",
    "\n",
    "        policy_loss = torch.mean(-traj_data.log_probs * advantage.detach())  # replace\n",
    "\n",
    "        loss = value_loss + policy_loss\n",
    "        # end student code\n",
    "        return loss\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        logits = self.policy(obs)\n",
    "        probs = categorical.Categorical(logits=logits)\n",
    "        actions = probs.sample()\n",
    "        return actions, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "O0T0KKk6jlTS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed: VPG Networks appear correct!\n",
      "Test passed: VPG loss appears correct!\n"
     ]
    }
   ],
   "source": [
    "# @title VPG Units Tests (must run VPG Agent cell above first)\n",
    "def VPG_networks():\n",
    "    a = Agent(32, 6)\n",
    "    assert (\n",
    "        a.name == \"VPG\"\n",
    "        and a.policy(torch.randn(64, 32)).shape == (64, 6)\n",
    "        and a.value(torch.randn(64, 32)).shape == (64, 1)\n",
    "        and isinstance(list(a.policy.children())[-1], nn.Linear)\n",
    "    ), f\"Networks not initialized correctly\"\n",
    "    print(\"Test passed: VPG Networks appear correct!\")\n",
    "\n",
    "\n",
    "VPG_networks()\n",
    "\n",
    "\n",
    "def VPG_loss():\n",
    "    n_steps, n_envs, n_obs, n_actions = 10, 5, 4, 1\n",
    "    traj_data = TrajData(n_steps, n_envs, n_obs, n_actions)\n",
    "    torch.manual_seed(0)\n",
    "    traj_data.states = torch.rand_like(traj_data.states)\n",
    "    traj_data.actions = torch.randint(0, n_actions, traj_data.actions.shape)\n",
    "    traj_data.rewards = torch.rand_like(traj_data.rewards)\n",
    "    traj_data.not_dones = torch.randint(0, 2, traj_data.not_dones.shape)\n",
    "    traj_data.log_probs = torch.rand_like(traj_data.log_probs)\n",
    "    traj_data.returns = torch.rand_like(traj_data.returns)\n",
    "    a = Agent(4, 1)\n",
    "    torch.manual_seed(0)\n",
    "    a.policy = nn.Linear(4, 1)\n",
    "    a.value = nn.Linear(4, 1)\n",
    "    assert abs(a.get_loss(traj_data).item() - 0.0618) < 1e-4, (\n",
    "        \"VPG loss does not match expected value.\"\n",
    "    )\n",
    "    print(\"Test passed: VPG loss appears correct!\")\n",
    "\n",
    "\n",
    "VPG_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8WVzb5BMGu2"
   },
   "source": [
    "Run the VPG Agent cell above, and then run the rollout/update cell below. <br> Scroll back up to tensorboard and refresh (circular white arrow in the right of the orange bar) to visualize your reward curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WnAiItxlrJ7Q"
   },
   "outputs": [],
   "source": [
    "drl = DRL()\n",
    "for i in range(250):\n",
    "    drl.rollout(i)\n",
    "    drl.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtgMGojY6Qsb"
   },
   "source": [
    "#### VPG Conceptual Question:\n",
    "In 2 or 3 sentences, why might subtracting a value network baseline improve performance of our RL agent? (Hint: policy gradient slides) Based on the tensorboard curves, what is the effect in this environment? Why? <br>\n",
    "\n",
    "Type answer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbzNmNLMMFTj"
   },
   "source": [
    "--------------------------------------------------------------------------------\n",
    "## 3. **Proximal Policy Optimization** <br> (aka REINFORCE with Baseline and Clipped Surrogate Objective) [16 pts + Extra Credit 4 pts]\n",
    "\n",
    "1.   Define your networks [1 pts]\n",
    "  *   Value network (same as VPG)\n",
    "  *   Policy network (same as VPG)\n",
    "\n",
    "2.   Define your losses [5 pts]\n",
    "  *   Value loss (same as VPG)\n",
    "  *   Policy loss (the heart of PPO)\n",
    "  *   Add them\n",
    "\n",
    "3. Conceptual Questions [5 + 5 pts]\n",
    "\n",
    "3. Generalized Advantage Estimation (GAE) [Extra Credit: 4 pts]\n",
    "--------------------------------------------------------------------------------\n",
    "HINTS:\n",
    "\n",
    "#### Policy Loss\n",
    "\n",
    "Our PPO policy loss still uses the advantage defined in VPG:$$A(s_t, a_t)  = A_t = R_t - V_{\\theta}(s_t)$$\n",
    "\n",
    "But we maximize a clipped surrogate objective which is designed to keep policy updates bounded:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{clip}}(\\theta) = \\mathbb{E}_t \\left[ \\min \\left( \\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)} \\cdot A_t, \\text{clip}\\left(\\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)}, 1 - \\epsilon, 1 + \\epsilon\\right) \\cdot A_t \\right) \\right].\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ \\pi_\\theta(a_t | s_t) $: the probability of taking action $a_t$ in state $s_t$ under the current policy with parameters $ \\theta $.\n",
    "- $ \\pi_{\\theta_{\\text{old}}}(a_t | s_t) $: the probability of taking action $a_t$ in state $s_t$ under the old policy before the update.\n",
    "- $ A_t $: the advantage estimate at timestep $t$.\n",
    "- $ \\epsilon $: the clip range hyperparameter that limits policy updates.\n",
    "- $ \\text{clip}(x, 1 - \\epsilon, 1 + \\epsilon) $: clips $x$ to the range $[1 - \\epsilon, 1 + \\epsilon]$ to ensure conservative updates.\n",
    "\n",
    "<br>\n",
    "\n",
    "Lets break it down.\n",
    "\n",
    "\n",
    "*   First, $\\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)}$ is the probability ratio between the policy being updated and the policy that was rolled out to collect the training data (traj_data). It is only meaningful when multiple epochs of training are performed on the training data from a single rollout. Indeed, in the first epoch, the current and old policies are the same so the ratio will be one.\n",
    "\n",
    "*   For numerical stability, we leverage a basic property of logs ($\\frac{A}{B} = \\exp (\\log A - \\log B )$), and we calculate this ratio as\n",
    "\n",
    "$$\n",
    "\\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)} = \\exp \\left( \\log \\pi_\\theta(a_t | s_t) - \\log \\pi_{\\theta_{\\text{old}}}(a_t | s_t) \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "*   Conceptually, defining policy loss as the product $\\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)} \\cdot A_t$ is enough to train a policy. Feel free to try it and view your learning results in tensorboard.\n",
    "\n",
    "*   However, after several epochs of training, the new policy probabilities $\\pi_\\theta(a_t | s_t)$ may deviate so far from the old policy probabilities $\\pi_{\\theta_{\\text{old}}}(a_t | s_t)$, that the advantage data from the rollout (traj_data) is no longer valid. This can cause catastropic collapse in the policy.\n",
    "\n",
    "*   Enter $\\text{clip}\\left(\\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)}, 1 - \\epsilon, 1 + \\epsilon\\right)$, which never lets the probability ratio become smaller than $1 - \\epsilon$ or larger than $1 + \\epsilon$, where common values of $\\epsilon$ are .2, .1, or .05. It's applied pointwise across all $(s, a)$ pairs.\n",
    "\n",
    "*   Finally, by taking the pointwise minimum of the unclipped and clipped products across all $(s, a)$ pairs, we ensure the largest policy update possible is made, while remaining conservatively close to the old policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uozaYAZutDhk"
   },
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, n_obs, n_actions):\n",
    "        super().__init__()\n",
    "        self.name = \"PPO\"\n",
    "\n",
    "        torch.manual_seed(0)  # needed before network init for fair comparison\n",
    "\n",
    "        # todo: student code here\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(n_obs, HIDDEN_WIDTH),\n",
    "            nn.Linear(HIDDEN_WIDTH, HIDDEN_WIDTH),\n",
    "            nn.Linear(HIDDEN_WIDTH, n_actions),\n",
    "        )  # replace\n",
    "\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(n_obs, HIDDEN_WIDTH),\n",
    "            nn.Linear(HIDDEN_WIDTH, HIDDEN_WIDTH),\n",
    "            nn.Linear(HIDDEN_WIDTH, 1),\n",
    "        )  # replace\n",
    "        # end student code\n",
    "\n",
    "    def get_loss(self, traj_data, epsilon=0.1):\n",
    "        # todo: student code here\n",
    "\n",
    "        loss = None  # replace\n",
    "        # end student code\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        logits = self.policy(obs)\n",
    "        probs = categorical.Categorical(logits=logits)\n",
    "        actions = probs.sample()\n",
    "        return actions, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vdQ_b4e3shhy"
   },
   "outputs": [],
   "source": [
    "# @title PPO Unit Tests (must run PPO Agent cell above first)\n",
    "def PPO_networks():\n",
    "    a = Agent(12, 7)\n",
    "    assert (\n",
    "        a.name == \"PPO\"\n",
    "        and a.policy(torch.randn(128, 12)).shape == (128, 7)\n",
    "        and a.value(torch.randn(4, 12)).shape == (4, 1)\n",
    "        and isinstance(list(a.policy.children())[-1], nn.Linear)\n",
    "    ), f\"Networks not initialized correctly\"\n",
    "    print(\"Test passed: PPO Networks appear correct!\")\n",
    "\n",
    "\n",
    "PPO_networks()\n",
    "\n",
    "\n",
    "def PPO_loss():\n",
    "    n_steps, n_envs, n_obs, n_actions = 10, 5, 4, 1\n",
    "    traj_data = TrajData(n_steps, n_envs, n_obs, n_actions)\n",
    "    torch.manual_seed(0)\n",
    "    traj_data.states = torch.rand_like(traj_data.states)\n",
    "    traj_data.actions = torch.randint(0, n_actions, traj_data.actions.shape)\n",
    "    traj_data.rewards = torch.rand_like(traj_data.rewards)\n",
    "    traj_data.not_dones = torch.randint(0, 2, traj_data.not_dones.shape)\n",
    "    traj_data.log_probs = torch.rand_like(traj_data.log_probs)\n",
    "    traj_data.returns = 2 * torch.rand_like(traj_data.returns) - 1\n",
    "    a = Agent(4, 1)\n",
    "    torch.manual_seed(0)\n",
    "    a.policy = nn.Linear(4, 1)\n",
    "    a.value = nn.Linear(4, 1)\n",
    "    assert abs(a.get_loss(traj_data).item() - 0.9314) < 1e-4, (\n",
    "        \"PPO loss does not match expected value.\"\n",
    "    )\n",
    "    print(\"Test passed: PPO loss appears correct!\")\n",
    "\n",
    "\n",
    "PPO_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkStaFnEMha7"
   },
   "source": [
    "Run the PPO cell above, and then run this cell, to plot results in tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k4yc9VZ0vyLd"
   },
   "outputs": [],
   "source": [
    "drl = DRL()\n",
    "for i in range(250):\n",
    "    drl.rollout(i)\n",
    "    drl.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-m4-XfO48lrk"
   },
   "source": [
    "#### PPO Conceptual Questions:\n",
    "\n",
    "*   If advantage for a state-action pair $A(s_t, a_t)$ is a large positive number and epsilon is $\\epsilon = .2$, how might the probability ratio for $(s_t, a_t)$ evolve over 10 training epochs with a large learning rate? What if Advantage is large and negative? What if its zero? <br>\n",
    "\n",
    "\n",
    "*   When the clipped expression is activated for a given state-action pair, what is the gradient of the loss function with respect to network parameters for that state action pair? How will the probability of that action be changed during back propagation? \n",
    "$$\n",
    "\\text{clip}\\left(\\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)}, 1 - \\epsilon, 1 + \\epsilon\\right)  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUuZXJ_-Gc8q"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GmiAswbO6VbA"
   },
   "outputs": [],
   "source": [
    "untrained = DRL()\n",
    "visualize(untrained.agent)\n",
    "print(\"untrained agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iVZCJ1yBZwRe"
   },
   "outputs": [],
   "source": [
    "# each time this cell is run, a new random rollout is recorded\n",
    "# put this cell below REINFORCE or VPG and re-run their training if youd like to visualize them.\n",
    "visualize(drl.agent)\n",
    "print(\"PPO trained agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3XTIdnpD0Xu"
   },
   "source": [
    "#### Optional Extra Credit: GAE [4 pts]\n",
    "If you have time and you're up for a challenge... Implement Generalized Advantage Estimation for PPO  or VPG instead of our simple sum of discounted rewards. Plot the reward curves in tensorboard.\n",
    "\n",
    "Please modify the code below to calcualte the advantage using GAE. You must maintain the original functionality previously implemented, but besides that modify the code how ever you see fit. Copy-paste code below or modify it in place. For GAE, disregard these warnings: **You shouldnt need to change this code**. You will probably need to modify TrajData, DRL.rollout(), and implement a new Agent. Note, we havent tested how easy or hard these changes are, so proceed with caution.\n",
    "\n",
    "Here's a great lecture by Sergey Levine on [Eligability traces and GAE](https://www.youtube.com/watch?v=quRjnkj-MA0), especially starting from 7:41. Good luck and happy coding!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2D99rzhbm4bA"
   },
   "outputs": [],
   "source": [
    "def calc_gae(self, values, gamma, lam):\n",
    "    # todo: student code here\n",
    "    self.advantage = None  # replace\n",
    "    # end student code"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
